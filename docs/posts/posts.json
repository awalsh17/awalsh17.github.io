[
  {
    "path": "posts/2022-09-04-what-pharma-companies-tweet-about-data-science/",
    "title": "Which pharma companies tweet about data science?",
    "description": "Mining some twitter data for trends and differences.",
    "author": [
      {
        "name": "Alice Walsh",
        "url": {}
      }
    ],
    "date": "2022-09-04",
    "categories": [],
    "contents": "\nI am interested in the rise of data science across all industries and\nthe growing demand for data-intensive work. In particular, I am\ninterested in pharmaceutical development (where I work) for a couple of\nreasons:\nData-intensive work is not “new” to Pharmaceutical development.\nBiostats teams are essential to designing clinicial trials, analyzing\nthese trials, and reporting the results. Likewise, research teams in\npreclinical R&D have also generated and processed large data sets\nfor many years.\nI get a lot of ads/tweets/etc. about the potential of AI/ML/Data for\ndrug discovery and development. I am in the optimistic camp when it\ncomes to how better technology and methods can unlock scientific\nbreakthroughs. But, I am also weary of unrealistic expectations and the\nabuse of “AI” to seem innovative or as a marketing tool.\nHere I looked at twitter data as a publicly-available data source to\nconfirm or reject my hypotheses around\nWhich companies are promoting AI/ML/Data as part of their\nmarketing\nWhether the use of data science terms has increased in the last few\nyears\nMethods\nI used the great {rtweet} package to\ncollect recent tweets from companies official twitter accounts. It had\nbeen several years since I used this package, but thankfully, my old\ncode worked fine. I would recommend Will\nChase’s post on this topic as an intro, which I found very helpful\nwhen I got started.\nMost companies have multiple twitter accounts. I tried to be fair and\nuse each company’s “main” account, but sometimes there was also a\n“science at xyz” account that seemed interesting. So I also looked at\nthose.\nI chose a list of top\nPharma companies based on sales. I manually found their twitter\nhandles.\nI also added a representative public biotech, Recursion, to serve as\na comparison for a company that I expected to have a lot of content\nabout data and machine learning.\n\n\nph_usernames <- c(\n  \"janssenglobal\",\n  \"pfizer\", \n  \"roche\",\n  \"abbvie\",\n  \"novartis\", \"novartisscience\",\n  \"merck\",\n  \"bmsnews\", \"scienceatbms\",\n  \"gsk\",\n  \"sanofi\", \"sanofiscience\",\n  \"astrazeneca\",\n  \"takedapharma\",\n  \"lillypad\",\n  \"recursionpharma\") \n\n\n\nI retrieved the last 3,200 tweets from each account.\n\n\nph_tweets <- get_timeline(ph_usernames, n=3200)\n\n\n\n\n\n\nThis is a lot of tweets, but some are retweets.\n\n\nnrow(ph_tweets)\n\n\n[1] 47977\n\nI combined the tweet data for companies where I queried both the\n“main” account and the “science” account.\n\n\nph_tweets <- ph_tweets %>% \n  mutate(screen_name = case_when(\n    screen_name %in% c(\"bmsnews\",\"ScienceAtBMS\") ~ \"bmsnews|ScienceAtBMS\",\n    screen_name %in% c(\"Novartis\",\"NovartisScience\") ~ \"Novartis|NovartisScience\",\n    screen_name %in% c(\"sanofi\", \"SanofiScience\") ~ \"Sanofi|SanofiScience\",\n    TRUE ~ screen_name\n  ))\n\n\n\nWho tweeted about data\nscience the most?\nI can calculate the number of tweets per company that contain certain\nwords.\n\n\ncount_tweets_by_word <- function(data, search_words) {\n  data %>% \n    # no retweets\n    filter(!is_retweet) %>%\n    # remove urls\n    mutate(text_filt = \n             stringr::str_replace_all(text,\"https?://t.co/[A-Za-z\\\\d]+|&amp;\", \"\")) %>% \n    mutate(tweet_pos = stringr::str_detect(text_filt, \n                                           paste(search_words, collapse = \"|\"))) %>% \n    group_by(screen_name) %>% \n    summarise(n_pos = sum(tweet_pos),\n              n_neg = sum(!tweet_pos), \n              .groups = \"drop\") %>%\n    mutate(perc_pos = scales::percent(n_pos / (n_pos + n_neg))) %>% \n    arrange(desc(n_pos / (n_pos + n_neg)))\n}\n\n\n\nLet’s test with “data”.\n\n\ncount_tweets_by_word(ph_tweets, search_words = \"[Dd]ata\") %>% \n  DT::datatable(rownames = FALSE,\n                colnames = c(\"Twitter Handle\", \"Tweets w/ word\",\n                             \"Tweets w/o word\", \"Percent w/ word\"))\n\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"RecursionPharma\",\"Roche\",\"JanssenGlobal\",\"TakedaPharma\",\"bmsnews|ScienceAtBMS\",\"abbvie\",\"Novartis|NovartisScience\",\"AstraZeneca\",\"Sanofi|SanofiScience\",\"LillyPad\",\"Merck\",\"GSK\",\"pfizer\"],[251,403,248,154,162,128,224,106,170,84,82,77,67],[1518,2717,2426,2863,3302,2947,5178,2770,4922,2972,2970,2882,2995],[\"14.189%\",\"12.917%\",\"9.274%\",\"5.104%\",\"4.677%\",\"4.163%\",\"4.147%\",\"3.686%\",\"3.339%\",\"2.749%\",\"2.687%\",\"2.602%\",\"2.188%\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>Twitter Handle<\\/th>\\n      <th>Tweets w/ word<\\/th>\\n      <th>Tweets w/o word<\\/th>\\n      <th>Percent w/ word<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2]}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\nWe can get more specific and count tweets with the key words “data\nscience”, “AI”, and “machine learning”. I went ahead and included\n“statistics,” but sadly, there were very few tweets containing that\nword.\n\n\nsearch_words <- c(\"[Dd]ata [Ss]cience\",\n                  \"[Mm]achine [Ll]earning\",\n                  \"[ #]AI \",\n                  \"[Ss]tatistics\")\ncount_tweets_by_word(ph_tweets, search_words = search_words) %>% \n  DT::datatable(rownames = FALSE,\n                colnames = c(\"Twitter Handle\", \"Tweets w/ word\",\n                             \"Tweets w/o word\", \"Percent w/ word\"))\n\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"RecursionPharma\",\"AstraZeneca\",\"JanssenGlobal\",\"GSK\",\"Novartis|NovartisScience\",\"Roche\",\"bmsnews|ScienceAtBMS\",\"TakedaPharma\",\"Sanofi|SanofiScience\",\"abbvie\",\"Merck\",\"LillyPad\",\"pfizer\"],[209,46,40,35,37,19,21,16,26,8,4,3,2],[1560,2830,2634,2924,5365,3101,3443,3001,5066,3067,3048,3053,3060],[\"11.8146%\",\"1.5994%\",\"1.4959%\",\"1.1828%\",\"0.6849%\",\"0.6090%\",\"0.6062%\",\"0.5303%\",\"0.5106%\",\"0.2602%\",\"0.1311%\",\"0.0982%\",\"0.0653%\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>Twitter Handle<\\/th>\\n      <th>Tweets w/ word<\\/th>\\n      <th>Tweets w/o word<\\/th>\\n      <th>Percent w/ word<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2]}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\nLet’s check out some example tweets. Pfizer only had two data science\ntweets in our search:\n\n\nWe’re leading the charge in\n#AI\nwith the development of EstimATTR. This tool aims to educate healthcare\nproviders about combinations of cardiac and non-cardiac conditions known\nto be associated with wild-type ATTR-CM.\n\n— Pfizer Inc. (@pfizer)\nJanuary\n9, 2021\n\nAstraZeneca had 46 tweets. Let’s check out their most recent tweet\nabout machine learning:\n\n\nMachine learning allows us to identify new targets for novel medicines.\nOur scientists are applying these methods and accelerating cancer drug\ndiscovery by combining CRISPR and AI.\n#ICML2022\nhttps://t.co/KXemq7ytc4\npic.twitter.com/IwdxSoLa6h\n\n— AstraZeneca (@AstraZeneca)\nJuly\n17, 2022\n\nOverall, some companies are tweeting more about data science\nthan other companies! However, the total number and the\nfraction of total tweets that are about data science are tiny.\nFor AstraZeneca (the top tweeter), what were the top words they used\noverall?\n\n\nplot_top_words(\"astrazeneca\")\n\n\n\n\nWhen did\neveryone start tweeting about data science?\nI wanted to look at trends over time, but I was guessing that this\nwould not be feasible given the limitation of the twitter API returning\nthe most recent 3,200 tweets.\nHowever, the oldest tweets returned are from 2014, so I decided to do\na quick look. This is a bit lazy as the earliest tweet available from\neach company varies because they tweet at different frequencies:\nRecursion and Novartis less frequently and Janssen more frequently.\nConsidering the big pharma companies, the first appearance of the\nterms “data science”, “machine learning”, “AI”, or “statistics” was in\n2017.\nThe trend for these terms was increasing until 2020. I can speculate\nthat there may have been a lot of COVID-related tweets in 2020 and 2021,\nbut the data science tweets are back more than ever before in 2022!\n\n\nShow code\n\nph_tweets %>% \n  # filter for our interesting tweets\n  filter(screen_name != \"RecursionPharma\") %>%\n  mutate(text_filt = \n           stringr::str_replace_all(text, \"https?://t.co/[A-Za-z\\\\d]+|&amp;\", \"\")) %>% \n  mutate(text_pos = stringr::str_detect(text_filt, \n                                        paste(search_words, collapse = \"|\"))) %>%\n  # aggregate by year\n  group_by(text_pos,\n           year = lubridate::floor_date(created_at, unit = \"year\")) %>% \n  summarise(tweets = n(),\n            .groups = \"drop\") %>%\n  tidyr::complete(text_pos, year, \n                  fill = list(tweets = 0)) %>%\n  group_by(year) %>% \n  summarise(tweets = tweets[text_pos] / sum(tweets),\n            .groups = \"drop\") %>% \n  ggplot(aes(x=year, y=tweets)) + \n  geom_col() +\n  labs(title = \"Prevalence of tweets about data science\",\n       subtitle = \"Recent tweets from 12 pharma companies\",\n       y = \"Fraction of all tweets\")\n\n\n\n\n\nI also looked at the number of likes and retweets, but I didn’t identify\nany interesting patterns.\nFinally, I can break the tweets down by company. These are the\nresults for the companies with at least ten tweets with my key\nwords.\nWhile AstraZeneca was sending a lot of data science flavored tweets\nin 2019, there aren’t many tweets on the subject for the last 3 years.\nMeanwhile, Janssen has picked up the torch and is on track to tweet on\ndata science topics more than anyone this year.\nThese numbers are very small and so you shouldn’t draw any serious\nconclusions. I am guessing these changes could be explained by a change\nof staff on the communications team rather than a major company\nstrategy!\n\n\nShow code\n\ntop_cos <- count_tweets_by_word(ph_tweets, \n                                search_words = search_words) %>% \n  filter(n_pos > 9) %>% pull(screen_name)\n\nph_tweets %>% \n  # filter for our interesting tweets\n  filter(screen_name %in% top_cos) %>%\n  mutate(text_filt = \n           stringr::str_replace_all(text, \"https?://t.co/[A-Za-z\\\\d]+|&amp;\", \"\")) %>% \n  filter(stringr::str_detect(text_filt, \n                             paste(search_words, collapse = \"|\"))) %>%\n  # aggregate by year & company\n  group_by(\n           year = lubridate::floor_date(created_at, unit = \"year\"),\n           screen_name) %>% \n  summarise(tweets = n(),\n            .groups = \"drop\") %>%\n  ggplot(aes(x=year, y=tweets, color = screen_name)) + \n  geom_point() + geom_line() + \n  facet_wrap(~screen_name, scales = \"free_y\") +\n  labs(title = \"Number of tweets about data science by pharma company\",\n       y = \"Number\") + \n  theme(legend.position = \"none\")\n\n\n\n\nWhat’s next?\nThis project was a fun thing to look at over a long weekend. It\nconfirmed my suspicions that some companies overall tweet more than\nothers, and that some companies are promoting their data science more\nthan others.\nI hope to post more on this topic by looking at job descriptions,\nLinkedIn, and clinical trial data to examine more deeply the impact of\ndata-intensive work on pharma R&D.\nsessionInfo\n\n\npander::pander(sessionInfo())\n\n\nR version 4.0.5 (2021-03-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale:\nen_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats,\ngraphics, grDevices, utils,\ndatasets, methods and base\nother attached packages: tidytext(v.0.3.1),\nggplot2(v.3.3.6), dplyr(v.1.0.9) and\nrtweet(v.0.7.0)\nloaded via a namespace (and not attached):\ntidyselect(v.1.1.2), xfun(v.0.31),\nbslib(v.0.2.5.1), pander(v.0.6.3),\npurrr(v.0.3.4), lattice(v.0.20-41),\ncolorspace(v.2.0-3), vctrs(v.0.4.1),\ngenerics(v.0.1.3), htmltools(v.0.5.1.1),\nSnowballC(v.0.7.0), yaml(v.2.3.5),\nutf8(v.1.2.2), rlang(v.1.0.4),\njquerylib(v.0.1.4), pillar(v.1.8.0),\nglue(v.1.6.2), withr(v.2.5.0), DBI(v.1.1.1),\nlifecycle(v.1.0.1), stringr(v.1.4.0),\nmunsell(v.0.5.0), gtable(v.0.3.0),\nhtmlwidgets(v.1.5.3), memoise(v.2.0.0),\nevaluate(v.0.15), labeling(v.0.4.2),\nknitr(v.1.39), fastmap(v.1.1.0),\ncrosstalk(v.1.1.1), curl(v.4.3.2),\nfansi(v.1.0.3), highr(v.0.9),\ntokenizers(v.0.2.1), Rcpp(v.1.0.9),\nopenssl(v.2.0.2), scales(v.1.2.0),\nDT(v.0.18), cachem(v.1.0.5),\njsonlite(v.1.8.0), farver(v.2.1.1),\ndistill(v.1.3), askpass(v.1.1),\ndigest(v.0.6.29), stringi(v.1.7.8),\ngrid(v.4.0.5), cli(v.3.3.0), tools(v.4.0.5),\nmagrittr(v.2.0.3), sass(v.0.4.0),\ntibble(v.3.1.8), janeaustenr(v.0.1.5),\ncrayon(v.1.5.1), tidyr(v.1.2.0),\npkgconfig(v.2.0.3), downlit(v.0.4.0),\nellipsis(v.0.3.2), Matrix(v.1.3-2),\nlubridate(v.1.8.0), assertthat(v.0.2.1),\nrmarkdown(v.2.11), httr(v.1.4.3),\nrstudioapi(v.0.13), R6(v.2.5.1) and\ncompiler(v.4.0.5)\n\n\n\n\n",
    "preview": "posts/2022-09-04-what-pharma-companies-tweet-about-data-science/what-pharma-companies-tweet-about-data-science_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2022-09-04T12:54:10-04:00",
    "input_file": "what-pharma-companies-tweet-about-data-science.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-07-26-rstudio-conference-talk-2022/",
    "title": "RStudio Conference Talk 2022",
    "description": "Becoming creative: How I designed a quilt with R.",
    "author": [
      {
        "name": "Alice Walsh",
        "url": {}
      }
    ],
    "date": "2022-07-26",
    "categories": [],
    "contents": "\nUpdate: The talk recording is now available online on the conference\nwebsite.\nI am excited to attend my first in person R-centric conference this\nyear with RStudio::conf.\nI am even more excited to get the opportunity to participate as a\nspeaker.\nLink to slides\nAbstract\n\nWhen someone asks about essential skills for data careers, I often\nhear responses like R, Python, and machine learning. However, I argue\nthat creativity is an underrated skill that you can and should practice.\nIn this talk, I want to tell you a story about a project I did to\nstretch my creative brain and use my favorite tool, R. I designed a\nquilt in R using generative art ideas. Then I created individual blocks\nthat make up the larger design. I used foundation paper piecing, a\nmethod that allows for intricate designs but has geometrical\nconstraints. I hope my talk will entertain and inspire folks to exercise\ntheir creative muscles to improve their performance and enjoyment of\ntheir day jobs.\n\nThanks to the organizers for having me and making this a very\npositive experience!\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-18T13:47:26-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-18-data-science-hangout/",
    "title": "Hanging out with data people",
    "description": "I co-hosted a \"Data Science Hangout\" in May.",
    "author": [
      {
        "name": "Alice Walsh",
        "url": {}
      }
    ],
    "date": "2022-06-18",
    "categories": [],
    "contents": "\nI was the “co-host” for a Data Science\nHangout a few weeks ago in May. These are weekly informal\nconversations for data science leaders hosted by Rachael Dempsey and\nRStudio. Thanks Rachael for inviting me! You can find the recording of\nthe event over on RStudio’s YouTube.\n\n\nRachael and RStudio did a fantastic job recapping the conversation in\nthe video description. However, I will add a rough outline of the topics\ncovered here.\n1. Optimizing remote work\nOne difference between remote work and in-office work is the\nreduction of thinking time away from screens. The book “Rest: Why You\nGet More Done When You Work Less” by Alex Soojung-Kim Pang was\nrecommended, and I plan to read it (ironically when I have more time and\nless work).\nI shared some ideas for effective remote brainstorming (a topic I am\ninterested in). One of the great benefits of remote teams is the ability\nto leverage technology to “level the playing field” for people of\ndifferent personality types, communication styles, and work\nlocations.\n2. Getting\nbetter data versus imputing missing data\nThere was a lot of good chat about imputing missing data and\nrecommendations for R packages. Unfortunately, the zoom chat is not\ncaptured in the YouTube replay - one of the benefits of joining\nlive!\nA good reminder here about not always trying to “fix” your analysis\nbut seeing if you can “fix” your data. Sometimes that missing data could\npoint to a problem in your data generation that is worth\ninvestigating.\n3. Conferences, tech,\nand leadership styles\nSomeone asked about good conferences to attend. There were some\nexcellent recommendations and another data point that many people are\nexcited about the possibility of attending in-person meetings\nagain.\nTech stacks and teams switching from one set of tools (SAS) to\nanother (R). There were some great tips from others who have been\nthrough this transition.\nResources to get started in data science. The recommendations\nincluded finding something you are excited about and choosing something\nto focus on.\nLeadership styles and communicating with stakeholders. My advice was\nto be opinionated and make it clear what you want your audience to take\naway from any analysis/presentation/report.\n4. Hiring (emphasis on remote\nhiring)\nMy aspiration is to make our interview experience better for\ncandidates. I have been doing remote interviews for a while now, but I\nstill struggle. I hope to continue to innovate and draw from the\ncollective wisdom about what works and what doesn’t.\nThe YouTube video has an excellent summary of this part. It seems\nthe consensus is split on whether a technical component (live interview,\ntake-home assignment, etc.) is valuable or not.\nOne idea I like is to have the candidate bring their own example\ncode/analysis and walk through it as part of the interview.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-06-18T13:05:44-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-05-how-does-one-come-up-with-new-ideas/",
    "title": "How does one come up with new ideas?",
    "description": "Useful tips I learned from the book \"Zig Zag\" by Keith Sawyer.",
    "author": [
      {
        "name": "Alice Walsh",
        "url": {}
      }
    ],
    "date": "2022-06-05",
    "categories": [],
    "contents": "\nA book that stuck in my head is “Zig Zag: The Surprising Path to Greater\nCreativity” by Keith Sawyer. The book is both a quick read and a\ndense resource that you could return to in the future when you are in a\ncreative rut or need to solve a problem.\nComing up with new ideas when you have a challenge can be profoundly\ndaunting. So I am very excited to find new ways to come up with ideas. I\nhave used the two techniques described here and found them worth\nsharing.\nMethod one: transform other\nideas\nThe first is the SCAMPER method. Scamper\nstands for Substitute, Combine,\nAdapt, Magnify or Modify,\nPut to other uses, Eliminate,\nRearrange or Reverse.\n\nA short Wikipedia journey has informed me that SCAMPER may have been\ncreated by Alex Faickney Osborn in the 1950s. He was an advertising\nexecutive and he is credited with inventing brainstorming.\nThis is a process to get new ideas by taking existing ones and\ntransforming them. You go through each letter in SCAMPER and identify\nideas that use that verb.\nHere is an example that I created from my own life. I recently\nstarted quilting, and I needed new ideas for quilt designs (rather than\nusing an existing pattern). Note that this example works well because I\nam creating a physical thing. This method is less effective for abstract\nproblems.\nPrompt: I need a new quilt design\nSubstitute: I could use different colors or fabrics in an\nexisting design\n\nIdea is not that exciting or new\nCombine: Merge a couple of different patterns. Or combine\ntwo things I know about, like painting and quilting or programming and\nquilting\n\nThis idea was fascinating to me and I have created a few different\ndesigns now using R\nAdapt: I could convert a cross stitch pattern to a quilt\npattern or try to convert a painting or photo into a pattern\n\nA good one! I have been drawing inspiration from paintings\nMagnify or modify: Make a small quilt block into a\nhuge block. Modify a classic block by rotating or transforming into a\ndifferent shape\n\nThis idea feels like a good seed that could be built upon and improved\nEliminate: Quilt on a panel or solid fabric\nwithout piecing\n\nAlso a solid idea, but too basic\nReverse: Decompose an existing pattern and rearrange\nit\n\nThis also fell into the less exciting bucket once I reviewed all the\nideas\nLooking at the above ideas, I love a lot of them! The next step was\nto prioritize these ideas and then improve them further.\nMethod two: an idea quota\nThe second method I have used is very simple. You choose a problem\nand come up with three (or more if you are ambitious!) new\nideas/solutions every day for a week. I love this method because it can\nalso be collaborative. You could give everyone on a team a prompt, and\nthen everyone can share all their ideas at the end of the week.\nAnother wonderful thing about this method is that it eliminates the\ntendency I have to use my first idea. This process helps me consider\nmore ideas and take the time to dive into the problem. It is also highly\nchallenging to add more ideas after coming up with ten ideas.\n\n\n\n",
    "preview": "posts/2022-06-05-how-does-one-come-up-with-new-ideas/images/noun-increase-creativity-4287123.png",
    "last_modified": "2022-06-18T12:54:36-04:00",
    "input_file": {},
    "preview_width": 1200,
    "preview_height": 1200
  },
  {
    "path": "posts/2022-05-24-management-resources/",
    "title": "Managing data teams: resources",
    "description": "A collection of links and book recommendations.",
    "author": [
      {
        "name": "Alice Walsh",
        "url": {}
      }
    ],
    "date": "2022-05-24",
    "categories": [],
    "contents": "\nI wanted to gather some thoughts on managing computational research\nteams in biopharma (sometimes encompassing bioinformatics, computational\nbiology, and data science). Mostly, I want to keep track of various\nlinks and books and grow the list over time.\nRecommendations\nResilient Management\nby Lara Hogan\nThis book is an excellent overview of how to be a manager written\nwith a first-time manager in mind. However, it contains lots of good\nreminders for those with management experience.\n\nThe\nManager’s Path by Camille Fournier\nAnother great book - written for the tech audience.\n\nJacqueline Nolis has a great blog\nThere are a set of posts on hiring from 2017, which are very\nthoughtful.\nDr. Nolis also co-authored Build\na Career in Data Science.\n\nThe BICEPS model\nof core needs\nThis is a valuable framework to apply whenever you or someone you\nwork with reacts strongly to a situation.\nYou may have also heard of SCARF, which is more or less the same\nthing.\n\nGrowth Mindset\n“Growth mindset” was a common theme in various trainings I attended\nin the past. Some people react negatively to this concept, but they also\nhave a closed mindset.\n\nLearn about and practice active listening\nMaybe it sounds cliché, but listening is a skill that we could all\npractice more. I can struggle as an extrovert to be quiet and\nlisten.\nI would love to find some good resources on this (books, videos,\netc.). I have read a few ebooks to try to find a recommendation, but\nthis might be a topic that is better addressed with training or\nvideos.\n\nRead about the “bring\nme a rock” phenomenon\nI am not sure of the exact origins of the term “bring me a\nrock”\n\nThe\nLeadership Pipeline by Ram Charan\nThis book has been around for a while. I found some of the advice\nless relevant to research organizations, but the general concepts can be\nhelpful.\n\nBeing Glue by Tanya Reilly\nThis (very influential) tech talk has an excellent summary of many\npeople’s experiences at work where they are not rewarded for valuable\nwork. It is written from the perspective of a software engineer, but it\napplies more broadly to “technical” work.\n“Every senior person in an organization should be aware of the less\nglamorous - and often less-promotable - work that needs to happen to\nmake a team successful.”\n\nI am looking\nto add more resources/books/podcasts on\nCareer development resources - I am looking for some good materials\nthat are freely available for helping others with career planning.\nHiring and recruiting - what works well\nOther thoughts\nLately, I have been considering whether there is room for people to\nswitch between people leadership and technical leadership in\ncomputational research. Can you change from people management to an\nindividual contributor and then back again? My experience has been that\nthis is a less common path in Big Pharma/biotech (the industry where I\nam most familiar). However, this flexible path seems more common in tech\ncompanies. Maybe it can be the future for us as well? Personally, I went\nfrom a role where I was 90% managing teams and doing various other\n“leadership things” to joining a small company where I needed to provide\na lot of research contributions and strategy while we got off the\nground. Am I viewed as a good leader if I am too “in the weeds”? I also\nknow people who are perfectly capable of people management roles (maybe\neven great at them!), but the technical work energizes them and gets\nthem excited to show up every Monday morning.\n\n\n\n",
    "preview": "posts/2022-05-24-management-resources/images/noun-books-3647451.png",
    "last_modified": "2022-05-24T19:14:50-04:00",
    "input_file": {},
    "preview_width": 1200,
    "preview_height": 1200
  },
  {
    "path": "posts/2022-04-15-aacr-recap-2022/",
    "title": "Recap of AACR 2022",
    "description": "My notes on the AACR annual meeting, April 2022.",
    "author": [
      {
        "name": "Alice",
        "url": {}
      }
    ],
    "date": "2022-04-15",
    "categories": [],
    "contents": "\n\n\n\n\nA sketch of my experience in the convention center.\nI attended the American Association of Cancer Research (AACR) annual meeting in New Orleans from April 10th-13th. This was my first in-person conference in over two years!\nI am recapping my take-aways here at two levels:\nOn the ground: snippets of interesting science I saw\nFrom the clouds: thoughts on return to in-person meetings and networking\nOn the ground:\nCanine oncology\nThere was a presence from canine oncology companies, Fidocure and PetDx as well as posters from veterinary oncologists from the University of Florida (abstract 5030) and the University of Pennsylvania (abstract 1344 and 1363) among others.\nAccording to Fidocure, there are 6 million new cancer diagnoses in dogs annually versus less than 2 million cases in humans.\nI am interested in where this research develops. Canines are an exciting model for human cancer from a genomic similarity and environment similarity point of view. It is attractive to imagine the shared benefit of finding drugs that work both in humans and dogs.\n\nData science, AI, computational biology, etc.\nOverall, I didn’t see anything super exciting or anything super depressing. There was a good balance of approaches advanced by both academic and industry teams.\nThere was a session chaired by Trey Ideker called “Interpreting and Building Trust in Artificial Intelligence Models.” that included talks from Eli Van Allen and Su-In Lee. I enjoyed reading this preprint from Dr. Lee’s group after the meeting.\nThere was another session chaired by Olga Troyanskaya called “Artificial Intelligence in Cancer Research and Care” that included talks from Dana Pe’er and John Quackenbush.\nI also attended a mini-symposium called “Emerging Topics in Computational Oncology,” chaired by Ben Greenbaum, and Ben Raphael which featured several interesting talks on a broad range of topics.\n\nBiomarkers\nI enjoyed the talks from former BMS colleagues (Jonathan Baden and Jaclyn Neely) who discussed comparing bTMB and tTMB, and biomarker work from Nivolumab HCC clinical studies (abstracts 2134 and 2145).\nFoundation Medicine and collaborators from the University of Minnesota presented a poster on their latest HRD model (abstract 1249). Their model used XGBoost on DNA sequencing features to predict homologous recombination deficiency (HRD) and showed the predicted HRD across tumor types and the concordance with other HRD biomarkers (e.g., gLOH). This work is similar to models recently published in a preprint from Tempus that uses RNA features to predict HRD.\nSpatial transcriptomics and single-cell ’omics continue to capture the imagination of cancer researchers. There were a lot of posters and talks. I am excited to see that the challenges encountered by early adopters seem to be getting resolved. I am optimistic that additional (clinically actionable) insights will emerge beyond the technical “we can make the data.”\n\nFrom the clouds:\nI need new glasses. I haven’t looked at a screen farther than 20 inches away in a long time.\nI missed everyone! It was great connecting with present, past, and (hopefully) future colleagues and collaborators. Unfortunately, we haven’t figured out how to make virtual conferences as effective for creating and building connections.\nHighlighting new voices versus established researchers. Sometimes it can be boring to see the same stars giving similar talks, and I tend to skip those. I prefer talking with folks at their posters and chatting with other people after the sessions. I am thankful to all the people who were so friendly and welcoming to discuss ideas with me.\nDiversifying cancer research\nAACR gives awards to recognize and fund travel for minority researchers, and I hope that we can do more to diversify cancer research (both the researchers and the patients).\nThere were some fantastic posters and talks that highlighted the need for more diverse clinical trials and the differences in cancer patients based on genetic ancestry.\nCoincidentally, on April 13th, the FDA published new guidance on improving diversity in clinical trial participants.\n\nsessionInfo\n\n\npander::pander(sessionInfo())\n\n\nR version 4.0.5 (2021-03-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, utils, datasets, methods and base\nloaded via a namespace (and not attached): Rcpp(v.1.0.7), fansi(v.0.4.2), digest(v.0.6.29), R6(v.2.5.0), jsonlite(v.1.7.2), magrittr(v.2.0.1), evaluate(v.0.14), highr(v.0.9), stringi(v.1.5.3), rlang(v.0.4.10), cachem(v.1.0.5), jquerylib(v.0.1.4), bslib(v.0.2.5.1), vctrs(v.0.3.7), rmarkdown(v.2.11), distill(v.1.3), tools(v.4.0.5), pander(v.0.6.3), stringr(v.1.4.0), xfun(v.0.30), yaml(v.2.2.1), fastmap(v.1.1.0), compiler(v.4.0.5), memoise(v.2.0.0), htmltools(v.0.5.1.1), knitr(v.1.37), downlit(v.0.4.0) and sass(v.0.4.0)\n\n\n\n\n",
    "preview": "posts/2022-04-15-aacr-recap-2022/images/aacr_floor.png",
    "last_modified": "2022-04-16T09:44:53-04:00",
    "input_file": {},
    "preview_width": 640,
    "preview_height": 640
  },
  {
    "path": "posts/2022-03-30-quilting-part-2/",
    "title": "Quilting with R, part 2",
    "description": "More about using R to make a quilt.",
    "author": [
      {
        "name": "Alice",
        "url": {}
      }
    ],
    "date": "2022-04-06",
    "categories": [],
    "contents": "\n\nContents\nDesigning a quilt in R\nThe challenge - going from png to fabric\nConverting the ggplot into a line drawing\nBreaking into pieces\nThe finished quilt top\nsessionInfo\n\n\n\nDesigning a quilt in R\nTo recap the previous post on this topic, I generated a quilt design in R using {ggplot2} and I was working through my overall plan:\nI wrote some functions to create a design using ggplot2 (post 1)\nI wrote some code to turn my image into something that could be printed as individual quilt blocks (this post)\nI made the actual quilt!\nI revisited how to make my process more general and implement something useful to design quilts that use foundation paper piecing\nIn this post, I will go through an overview of part 2.\nThe challenge - going from png to fabric\nOnce I had a design, I needed to figure out how to make the design into something that could actually be pieced together from fabric.\nThe approach I used is called foundation paper piecing. This is a technique that enables a quilter to make very intricate and precise designs by sewing fabric onto a piece of paper sequentially. This provides stability and precision by sewing along a printed line.\nFoundation paper piecing (FPP for short) allows a quilter to make some incredible creations! Here is a link to the winners of this year’s Quiltcon awards. The people’s choice winner (a portrait of the artist’s family) was made using FPP.\nTo create an entire FPP quilt, you need to break your larger design into smaller blocks that are “piece-able.” Here is the basic FPP “algorithm” (typically performed by hand):\nA hypothetical starting design (keep in mind that the final quilt will actually be reversed as you sew onto the “wrong” side):\n\n\n\nBreak the larger design into blocks (usually into blocks that can be sewn together into row or columns)\n\n\n\nTest whether the blocks are “piece-able” - not all shapes can be created!\n\n\n\nIf the blocks are not “piece-able” - break into smaller blocks\nCreate the seam lines on the block and label the sections by order\n\n\n\nI am planning to save more information on steps 3 and 4 for a future post.\nHere, I will just focus on how to do step 2 easily in R.\nConverting the ggplot into a line drawing\nMy previous function, save_my_image(), creates the design and can save an image.\n\n\ndesign <- save_my_image(out_path = NULL, # don't need to save it\n                   height_range = 10:40,\n                   width_range = 20:80,\n                   n_cubes = 3, \n                   n_second_color = 1,\n                   horizon_y = 80)\n\n\n\n\n\n\nI created a new function, create_pattern_pdf, that takes the data.frame and plots as line drawing with labeled polygons. The next section breaks down how that works.\nBreaking into pieces\nOne easy way to separate a large image into smaller blocks is just to use a program that can do tiled printing (such as Adobe Reader). However, that doesn’t give as much flexibility as you might like to select where to break up the image.\nBelow is how I turned a design - which was created on a 2D plane from (x,y) of (0,0) to (100,100) into a set of pdfs that were 8\" by 8\" to print.\nRescale to the size of the final quilt\n\n\nlibrary(dplyr, quietly = TRUE) # used for data manipulations\nlibrary(ggplot2) # for plotting\nfinal_quilt_size <- 40 # units are arbitrary (I think in inches)\nscale_factor <- final_quilt_size / 100 # my image was \"100\" wide\nwidth_blocks <- final_quilt_size / 5   # I wanted 5 across, makes nice 8\"\n\n\n\nCreate a data.frame with the start and end x,y for each block\n\n\ngrid <- tidyr::crossing(x = seq(0, width_blocks * 4, width_blocks),\n                        y = seq(0, width_blocks * 4, width_blocks)) %>%\n  mutate(x_end = x + width_blocks,\n         y_end = y + width_blocks) %>%\n  arrange(x, y)\n\nhead(grid)\n\n\n# A tibble: 6 x 4\n      x     y x_end y_end\n  <dbl> <dbl> <dbl> <dbl>\n1     0     0     8     8\n2     0     8     8    16\n3     0    16     8    24\n4     0    24     8    32\n5     0    32     8    40\n6     8     0    16     8\n\nAdd in some information that was not in the design created previously\n\n\n# Add in the horizontal line (I wanted it at y = 50)\nhorizontal <- data.frame(cube_id = c(\"\",\"\"),\n                         x = c(0, 100),\n                         y = c(50, 50)) %>%\n  mutate(x = x*scale_factor, y = y*scale_factor)\n# This ensures that the paths are \"closed\" without any open edges\nmissing_paths <- design %>% arrange(desc(cube_id))\n\n\n\nLoop through each block, plot and label, save out to a file. I tried to label each face an index to indicate color, but in my final design there were overlapping polygons, so colors needed to be manually checked for each block.\n\n\nfor (i in seq_len(nrow(grid))) {\n  block <- design %>%\n    bind_rows(missing_paths) %>%\n    arrange(cube_id) %>%\n    mutate(x = x*scale_factor, y = y*scale_factor) %>%\n    group_by(id) %>%\n    mutate(ave_x = mean(x), ave_y = mean(y)) %>%\n    group_by(value) %>%\n    mutate(color = cur_group_id()) %>%\n    ungroup() %>%\n    ggplot() +\n    # horizon line\n    geom_path(aes(x = x, y = y, group = cube_id),\n              data = horizontal,\n              color = \"black\",\n              size = 1) +\n    geom_polygon(aes(x = x, y = y, group = id),\n                 fill = \"white\", color = \"blue\",\n                 size = 1,\n                 alpha = 1) +\n    geom_text(aes(label = color, x = ave_x, y = ave_y)) +\n    coord_equal(clip = \"off\",\n                xlim = c(grid$x[i], grid$x_end[i]),\n                ylim = c(grid$y[i], grid$y_end[i]),\n                expand = FALSE) +\n    theme_void() +\n    theme(legend.position = \"none\",\n          panel.border = element_rect(colour = \"black\", fill=NA, size=0.5))\n  ggsave(plot = block,\n         filename = paste0(\"to_print/\",i,\".pdf\"),\n         width = width_blocks, height = width_blocks)\n}\n\n\n\n One trick to plot only certain limits of the image without removing data is to set the limits within coord_equal().  Note that I used theme_void() again to remove most the axis information and gridlines.  Further, I added back in a panel.border, which will be very useful when I go to make the blocks.\nFor illustration purposes, here is what one of the blocks looked like. Now I could print (at 100% scale) and the squares were a perfect 8 inches.\n\n\n\nThe finished quilt top\nHere is a preview of the completed quilt top. I still need to actually quilt it, but the top does look like the original design!\n\n\n\nThe code is available in a GitHub repo.\nsessionInfo\n\n\npander::pander(sessionInfo())\n\n\nR version 4.0.5 (2021-03-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, utils, datasets, methods and base\nother attached packages: ggplot2(v.3.3.5) and dplyr(v.1.0.5)\nloaded via a namespace (and not attached): Rcpp(v.1.0.7), bslib(v.0.2.5.1), compiler(v.4.0.5), pillar(v.1.6.0), jquerylib(v.0.1.4), highr(v.0.9), tools(v.4.0.5), digest(v.0.6.29), downlit(v.0.4.0), gtable(v.0.3.0), jsonlite(v.1.7.2), evaluate(v.0.14), memoise(v.2.0.0), lifecycle(v.1.0.0), tibble(v.3.1.0), pkgconfig(v.2.0.3), png(v.0.1-7), rlang(v.0.4.10), rstudioapi(v.0.13), cli(v.3.1.0), DBI(v.1.1.1), distill(v.1.3), yaml(v.2.2.1), xfun(v.0.30), fastmap(v.1.1.0), withr(v.2.4.2), stringr(v.1.4.0), knitr(v.1.37), generics(v.0.1.0), vctrs(v.0.3.7), sass(v.0.4.0), grid(v.4.0.5), tidyselect(v.1.1.0), glue(v.1.4.2), R6(v.2.5.0), fansi(v.0.4.2), rmarkdown(v.2.11), pander(v.0.6.3), tidyr(v.1.1.3), purrr(v.0.3.4), magrittr(v.2.0.1), scales(v.1.1.1), htmltools(v.0.5.1.1), ellipsis(v.0.3.1), assertthat(v.0.2.1), colorspace(v.2.0-0), utf8(v.1.2.1), stringi(v.1.5.3), munsell(v.0.5.0), cachem(v.1.0.5) and crayon(v.1.4.1)\n\n\n\n\n",
    "preview": "posts/2022-03-30-quilting-part-2/images/finished_top.png",
    "last_modified": "2022-04-06T17:26:54-04:00",
    "input_file": {},
    "preview_width": 1267,
    "preview_height": 1280
  },
  {
    "path": "posts/2022-03-24-quilting-with-r/",
    "title": "Quilting with R, part 1",
    "description": "One of my hobbies is quilting, so I designed a quilt in R.",
    "author": [
      {
        "name": "Alice",
        "url": {}
      }
    ],
    "date": "2022-03-24",
    "categories": [],
    "contents": "\n\nContents\nDesigning a quilt in R - the original idea\nMaking some aRt\nThe basic principle\nGetting random\n\nFor future work\nsessionInfo\n\n\nDesigning a quilt in R - the original idea\nI took up quilting as a creative outlet near the beginning of 2021. After completing a few projects, I decided to make some original designs.\nAt the same time, I was seeing a lot of amazing #Rtistry projects shared on Twitter.\nSo, I thought I would try to create some generative art in R and transform it into a quilt!\nThis endeavor turned into a large project, roughly divided into the following steps:\nI wrote some functions to create a design using ggplot2\nI wrote some code to turn my image into something that could be printed as individual quilt blocks\nI made the actual quilt!\nI revisited how to make my process more general and implement something useful to design quilts that use foundation paper piecing\nIn this post, I will go through an overview of part 1.\nMaking some aRt\nI was inspired by the art of Fred Kaplan, who was my instructor for a couple of continuing education painting classes. Fred has created some fantastic imagined landscapes that feature geometric structures.\nI thought I could create some interesting polygons drawn with 2-point perspective in ggplot2. Then, I could use fabrics that suggest a light source with lighter values on some faces and darker values on others.\nHere is my final design:\n\n\n\nThe basic principle\nThe first thing I set to do was program the creation of some rectangular polygons drawn with 2-point perspective.\nThis can be achieved with {ggplot2} using geoms like geom_path(), geom_segment(),or geom_polygon().\nFirst, load some packages.\n\n\nlibrary(dplyr, quietly = TRUE) # for data manipulation, pipe\nlibrary(ggplot2)               # plot engine\nlibrary(retistruct)            # to get intersection of lines\n\nset.seed(45)\n\n\n\nA box is just 7 or 9 segments. To draw a box, you need to specify\nAn x location for the left, center, and right segments\nA y location for the top and bottom on the center segment\nThe location of the two vanishing points (Here I set the first to (0,0) and the second to (some value, 0))\nYou calculate all the segments’ start and stop x and y coordinates from this information. See below the function, make_new_cube, that performs this calculation to make a single box.\n\n\nShow code\n\nmake_new_cube <- function(xes, # a vector of three values between (0, vp)\n                          yes, # a vector of two values\n                          vp = 10) {\n  # pick the vanishing points (y == 0)\n  vp <- list(c(0, 0), c(vp, 0))\n  # make the center vertical segment\n  cube <- data.frame(x = xes[2],\n                     xend = xes[2],\n                     y = yes[1],\n                     yend = yes[2])\n\n  # add the left vertical segment\n  new_row <- c(xes[1],\n               xes[1],\n               (yes[1]/xes[2])*xes[1],\n               (yes[2]/xes[2])*xes[1])\n  cube <- rbind(cube, new_row)\n\n  # add the right vertical segment\n  new_row <- c(xes[3],\n               xes[3],\n               (yes[1]/(xes[2] - vp[[2]][1]))*(xes[3] -xes[2]) + yes[1],\n               (yes[2]/(xes[2] - vp[[2]][1]))*(xes[3] -xes[2]) + yes[2])\n  cube <- rbind(cube, new_row)\n\n  # add the top lines\n  new_row <- c(cube[1,1], cube[2,1], cube[1,3], cube[2,3])\n  cube <- rbind(cube, new_row)\n  new_row <- c(cube[1,1], cube[3,1], cube[1,3], cube[3,3])\n  cube <- rbind(cube, new_row)\n\n  # add the bottom lines\n  new_row <- c(cube[1,1], cube[2,1], cube[1,4], cube[2,4])\n  cube <- rbind(cube, new_row)\n  new_row <- c(cube[1,1], cube[3,1], cube[1,4], cube[3,4])\n  cube <- rbind(cube, new_row)\n\n  # if all above or below y = 0,  then get bottom or top segments\n  add_top <- all(c(cube$y, cube$yend) < 0)\n  add_bottom <- all(c(cube$y, cube$yend) > 0)\n\n  if (add_top) {\n    # intersect left bottom [2,3] to right vp and right bottom [3,3] to left vp\n    left_top <- c(xes[1], max(cube[2,3], cube[2,4]))  #left top\n    right_p <- vp[[2]] # right vp\n    right_top <- c(xes[3], max(cube[3,3], cube[3,4]))  #right top\n    left_p <- vp[[1]] # left vp\n    poss_top <- line.line.intersection(left_top, right_p,\n                                       right_top, left_p,\n                                       interior.only = TRUE)\n    new_row <- c(poss_top[1], left_top[1], poss_top[2], left_top[2])\n    cube <- rbind(cube, new_row)\n    new_row <- c(poss_top[1], right_top[1], poss_top[2], right_top[2])\n    cube <- rbind(cube, new_row)\n  }\n  if (add_bottom) {\n    # intersect left bottom [2,3] to right vp and right bottom [3,3] to left vp\n    left_b <- c(xes[1], min(cube[2,3], cube[2,4]))  #left bottom\n    right_p <- vp[[2]] # right vp\n    right_b <- c(xes[3], min(cube[3,3], cube[3,4]))  #right bottom\n    left_p <- vp[[1]] # left vp\n    poss_bottom <- line.line.intersection(left_b, right_p,\n                                          right_b, left_p, interior.only = TRUE)\n    new_row <- c(poss_bottom[1], left_b[1], poss_bottom[2], left_b[2])\n    cube <- rbind(cube, new_row)\n    new_row <- c(poss_bottom[1], right_b[1], poss_bottom[2], right_b[2])\n    cube <- rbind(cube, new_row)\n  }\n  return(cube)\n}\n\n\n\nLet’s test this function. It makes a nice data frame with our segments.\n\n\nnew_xes <- c(1, 2, 3) #left, center, right\nnew_yes <- c(4, 6) # bottom, top of center\n\nnew_cube <- make_new_cube(new_xes, new_yes, vp = 10)\nnew_cube\n\n\n    x xend        y yend\n1 2.0    2 4.000000 6.00\n2 1.0    1 2.000000 3.00\n3 3.0    3 3.500000 5.25\n4 2.0    1 4.000000 2.00\n5 2.0    3 4.000000 3.50\n6 2.0    1 6.000000 3.00\n7 2.0    3 6.000000 5.25\n8 1.6    1 1.866667 2.00\n9 1.6    3 1.866667 3.50\n\nFor the purpose of illustration, I will label the points on the plot.\n\n\nnew_cube %>%\n  ggplot() +\n  geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_label(aes(x = x, y = y, label = paste0(\"(\",x,\",\",y,\")\"))) +\n  theme_void()\n\n\n\n\nGetting random\nTo draw polygons in R where the faces would be colored, I rewrote the above make_new_cube function to be make_new_poly. To make it easier to conceptualize, this function default to a view that goes from (0,0) to (100,100). I also used colors that would suggest a light source, with darker colors on one side and lighter colors on the other.\nI then wrote a function save_my_image that\nTakes as arguments\nthe number of boxes to make\nthe number of these you want to be in an alternate color\nthe height and width ranges of the boxes\nYou can also customize the location of the horizon line and the colors as desired\n\nFor each box, it picks a random value for inputs of make_new_poly within the input height and width ranges\nCreates the “sky” and “ground” and adds all the boxes\nDisplays the image and (optionally) saves a copy to a file\nReturns the data frame with all the polygons\nHere is an example:\n\n\ndesign <- save_my_image(out_path = NULL, # don't need to save it\n                   height_range = 10:40,\n                   width_range = 20:80,\n                   n_cubes = 3, \n                   n_second_color = 1,\n                   horizon_y = 80)\n\n\n\n\n\n\nAnd another one:\n\n\ndesign <- save_my_image(out_path = NULL, # don't need to save it\n                   height_range = 60:80,\n                   width_range = 20:30,\n                   n_cubes = 6, \n                   n_second_color = 2,\n                   horizon_y = 40)\n\n\n\n\n\n\nFor future work\nI would have liked to have implemented cast shadows, but I think it would require a major overhaul of my framework. Briefly, you could calculate all the points in 3D and then convert them to a 2D projection.\nI could better structure and document these functions to make them easier to extend.\nThe code is available on GitHub here.\nsessionInfo\n\n\npander::pander(sessionInfo())\n\n\nR version 4.0.5 (2021-03-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, utils, datasets, methods and base\nother attached packages: retistruct(v.0.6.3), ggplot2(v.3.3.5) and dplyr(v.1.0.5)\nloaded via a namespace (and not attached): Rcpp(v.1.0.7), bslib(v.0.2.5.1), compiler(v.4.0.5), pillar(v.1.6.0), jquerylib(v.0.1.4), highr(v.0.9), magic(v.1.6-0), tools(v.4.0.5), digest(v.0.6.29), downlit(v.0.4.0), gtable(v.0.3.0), jsonlite(v.1.7.2), evaluate(v.0.14), memoise(v.2.0.0), lifecycle(v.1.0.0), tibble(v.3.1.0), pkgconfig(v.2.0.3), png(v.0.1-7), rlang(v.0.4.10), DBI(v.1.1.1), distill(v.1.3), yaml(v.2.2.1), xfun(v.0.30), fastmap(v.1.1.0), withr(v.2.4.2), stringr(v.1.4.0), knitr(v.1.37), ttutils(v.1.0-1), htmlwidgets(v.1.5.3), generics(v.0.1.0), vctrs(v.0.3.7), sass(v.0.4.0), grid(v.4.0.5), tidyselect(v.1.1.0), glue(v.1.4.2), R6(v.2.5.0), fansi(v.0.4.2), rgl(v.0.108.3), rmarkdown(v.2.11), pander(v.0.6.3), farver(v.2.1.0), purrr(v.0.3.4), magrittr(v.2.0.1), scales(v.1.1.1), htmltools(v.0.5.1.1), ellipsis(v.0.3.1), abind(v.1.4-5), assertthat(v.0.2.1), colorspace(v.2.0-0), labeling(v.0.4.2), utf8(v.1.2.1), geometry(v.0.4.5), stringi(v.1.5.3), munsell(v.0.5.0), cachem(v.1.0.5) and crayon(v.1.4.1)\n\n\n\n\n",
    "preview": "posts/2022-03-24-quilting-with-r/images/cubes_8_red_blue.png",
    "last_modified": "2022-03-25T09:49:38-04:00",
    "input_file": {},
    "preview_width": 4000,
    "preview_height": 4000
  },
  {
    "path": "posts/2022-03-20-more-models/",
    "title": "Testing many models with grouped data",
    "description": "Another example of something I do a lot and forget how to do.",
    "author": [
      {
        "name": "Alice",
        "url": {}
      }
    ],
    "date": "2022-03-20",
    "categories": [],
    "contents": "\n\nContents\nMany models!\nAn example: penguin linear models\nAnother example: survival models\nsessionInfo\n\n\n\nMany models!\nI often have a situation where I am testing many hypotheses.\nHow I tested many models in R in the past was to use lapply or a loop. I don’t think there is any problem with that approach, I just really like using a pattern these days with grouped data using tidyverse packages.\nThe general pattern is\nMake the data long (if not already long)\nGroup and nest\nMutate to calculate your statistics\nUn-nest, filter, or select to get your desired output\n There is a great vignette on this topic from the {broom} package.\n\nAnother day, I will add a post on operations on pairwise combinations of variables ala the {corrr} package.\nFirst, load some packages.\n\n\nlibrary(tidyr, quietly = TRUE) # manipulating data and nesting\nlibrary(dplyr, quietly = TRUE) # general data and piping\nlibrary(purrr, quietly = TRUE) # i will use purrr::map\nlibrary(broom)                 # very good at reformatting model objects\nlibrary(palmerpenguins)        # for more fun data\nlibrary(survival)              # for time to event models\nlibrary(ggplot2)               # to make our plots\ntheme_set(theme_minimal(base_family = \"Avenir\")) # for plot appearance\n\n\n\nAn example: penguin linear models\nLet’s say we are interested in the association between all the numeric variables in the {palmerpenguins} penguin dataset and the species.\nIf you are not familiar with this dataset, the three penguin species have different features like bill depth, bill length, body mass, and flipper length.\n\n\nhead(penguins, 4)\n\n\n# A tibble: 4 x 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm\n  <fct>   <fct>              <dbl>         <dbl>             <int>\n1 Adelie  Torgersen           39.1          18.7               181\n2 Adelie  Torgersen           39.5          17.4               186\n3 Adelie  Torgersen           40.3          18                 195\n4 Adelie  Torgersen           NA            NA                  NA\n# … with 3 more variables: body_mass_g <int>, sex <fct>, year <int>\n\nHere, we can see that Gentoo are some big penguins and that Adelie penguins have shorter bill length.\n\n\nShow code\n\npenguins %>% \n  pivot_longer(cols = where(is.numeric)) %>% \n  mutate(name = stringr::str_replace_all(name, \"_\", \" \"),\n         name = stringr::str_wrap(name, width = 10)) %>%\n  ggplot(aes(x = species, y = value, fill = species)) + \n  geom_boxplot() + \n  scale_fill_manual(values = c(\"#0E89BA\",\"#85BAA1\",\"#C16E70\")) +\n  facet_wrap(~name, scales = \"free\", nrow = 1) + \n  labs(title = \"The penguin species are different\",\n       x = NULL) +\n  theme(\n    legend.position = \"none\",\n    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)\n  )\n\n\n\n\nAs a reminder, I will follow the same general pattern above\nMake the data long - pivot all the numeric variables\nGroup and nest - group by the variable name\nMutate to calculate your statistics - variable ~ species\nUn-nest, filter, or select to get your desired output\n Below, I calculated p-values and R-squared values using\nstats::lm()\nstats::anova()\nbroom::tidy()\n\nNote that you could also use broom::glance() to get R-squared\n\n\npenguins %>% \n  # tidyr functions to select all the numeric columns and \n  # create a `name` and `value` column\n  pivot_longer(cols = where(is.numeric)) %>% \n  group_by(name) %>% \n  # tidyr::nest to create a data frame where each level of the \n  # grouped variable has a single row, and all the other\n  # rows and columns are now in a single nested column, `data`\n  nest() %>% \n  # use purrr::map to create new nested columns with the objects\n  # returned from `lm`, `anova`, `broom::tidy`\n  mutate(lm_fit = map(data, \n                      ~ lm(value ~ species, data = .x)),\n         r2 = map_dbl(lm_fit, ~summary(.x)$r.squared),\n         anova = map(lm_fit, anova),\n         tidied = map(anova, tidy)) %>% \n  unnest(tidied) %>%\n  # this filter removes the rows with \"Residuals\"\n  filter(term == \"species\") %>%\n  select(-data, -lm_fit, -anova) %>% \n  knitr::kable(digits = 3)\n\n\nname\nr2\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\nbill_length_mm\n0.708\nspecies\n2\n7.194317e+03\n3597.159\n410.600\n0.00\nbill_depth_mm\n0.680\nspecies\n2\n9.039670e+02\n451.984\n359.789\n0.00\nflipper_length_mm\n0.778\nspecies\n2\n5.247328e+04\n26236.642\n594.802\n0.00\nbody_mass_g\n0.670\nspecies\n2\n1.468642e+08\n73432107.078\n343.626\n0.00\nyear\n0.003\nspecies\n2\n6.010000e-01\n0.300\n0.447\n0.64\n\nWe could also do this with group_modify in dplyr.\nFrom the documentation:\n\ngroup_map(), group_modify() and group_walk() are purrr-style functions that can be used to iterate on grouped tibbles.\n\n\n\npenguins %>% \n  pivot_longer(cols = where(is.numeric)) %>% \n  group_by(name) %>% \n  # there is a litle extra work here to return r.squared\n  # group_modify needs the returned value to be a data.frame!\n  # so you need to create one\n  group_modify( ~cbind(tibble(summary(lm(value ~ species, data = .))$r.squared,\n                              .name_repair = ~c(\"r2\")),\n                       tidy(anova(lm(value ~ species, data = .))) %>%\n                         filter(term == \"species\"))) %>%\n  knitr::kable(digits = 3)\n\n\nname\nr2\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\nbill_depth_mm\n0.680\nspecies\n2\n9.039670e+02\n451.984\n359.789\n0.00\nbill_length_mm\n0.708\nspecies\n2\n7.194317e+03\n3597.159\n410.600\n0.00\nbody_mass_g\n0.670\nspecies\n2\n1.468642e+08\n73432107.078\n343.626\n0.00\nflipper_length_mm\n0.778\nspecies\n2\n5.247328e+04\n26236.642\n594.802\n0.00\nyear\n0.003\nspecies\n2\n6.010000e-01\n0.300\n0.447\n0.64\n\nAnother example: survival models\nI often work with time to event models (survival models). You can also follow this same pattern.\nTake for example the survival::lung dataset that has some variables like age, sex, performance status (ECOG and Karnofsky), etc.\n\n\nglimpse(lung)\n\n\nRows: 228\nColumns: 10\n$ inst      <dbl> 3, 3, 3, 5, 1, 12, 7, 11, 1, 7, 6, 16, 11, 21, 12,…\n$ time      <dbl> 306, 455, 1010, 210, 883, 1022, 310, 361, 218, 166…\n$ status    <dbl> 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ age       <dbl> 74, 68, 56, 57, 60, 74, 68, 71, 53, 61, 57, 68, 68…\n$ sex       <dbl> 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1,…\n$ ph.ecog   <dbl> 1, 0, 0, 1, 0, 1, 2, 2, 1, 2, 1, 2, 1, NA, 1, 1, 1…\n$ ph.karno  <dbl> 90, 90, 90, 90, 100, 50, 70, 60, 70, 70, 80, 70, 9…\n$ pat.karno <dbl> 100, 90, 90, 60, 90, 80, 60, 80, 80, 70, 80, 70, 9…\n$ meal.cal  <dbl> 1175, 1225, NA, 1150, NA, 513, 384, 538, 825, 271,…\n$ wt.loss   <dbl> NA, 15, 15, 11, 0, 0, 10, 1, 16, 34, 27, 23, 5, 32…\n\nWe can repeat the same pattern to test cox proportional hazards models for these variables individually in univariate models.\n\n\nlung %>% \n  # tidyr to make the data long\n  pivot_longer(cols = -c(status, time)) %>% \n  group_by(name) %>% \n  # group the data\n  nest() %>% \n  # use purrr::map to create new nested columns with the objects\n  mutate(cox_fit = map(data, \n                      ~ coxph(Surv(time, status) ~ value, data = .x)),\n         tidied = map(cox_fit, tidy, conf.int = TRUE)) %>% \n  unnest(tidied) %>% \n  select(-data, -cox_fit) %>% \n  knitr::kable(digits = 3)\n\n\nname\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\ninst\nvalue\n-0.010\n0.010\n-0.942\n0.346\n-0.030\n0.010\nage\nvalue\n0.019\n0.009\n2.035\n0.042\n0.001\n0.037\nsex\nvalue\n-0.531\n0.167\n-3.176\n0.001\n-0.859\n-0.203\nph.ecog\nvalue\n0.476\n0.113\n4.198\n0.000\n0.254\n0.698\nph.karno\nvalue\n-0.016\n0.006\n-2.810\n0.005\n-0.028\n-0.005\npat.karno\nvalue\n-0.020\n0.005\n-3.631\n0.000\n-0.031\n-0.009\nmeal.cal\nvalue\n0.000\n0.000\n-0.535\n0.593\n-0.001\n0.000\nwt.loss\nvalue\n0.001\n0.006\n0.217\n0.828\n-0.011\n0.013\n\n\nYou could easily add a mutate() here to calculate p-values adjusted for multiple comparisons.\nNote: edited on 2022-04-08 to fix mistake with group_modify()\nsessionInfo\n\n\npander::pander(sessionInfo())\n\n\nR version 4.0.5 (2021-03-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, utils, datasets, methods and base\nother attached packages: ggplot2(v.3.3.5), survival(v.3.2-10), palmerpenguins(v.0.1.0), broom(v.0.7.6), purrr(v.0.3.4), dplyr(v.1.0.5) and tidyr(v.1.1.3)\nloaded via a namespace (and not attached): tidyselect(v.1.1.0), xfun(v.0.30), bslib(v.0.2.5.1), pander(v.0.6.3), splines(v.4.0.5), lattice(v.0.20-41), colorspace(v.2.0-0), vctrs(v.0.3.7), generics(v.0.1.0), htmltools(v.0.5.1.1), yaml(v.2.2.1), utf8(v.1.2.1), rlang(v.0.4.10), jquerylib(v.0.1.4), pillar(v.1.6.0), glue(v.1.4.2), withr(v.2.4.2), DBI(v.1.1.1), lifecycle(v.1.0.0), stringr(v.1.4.0), munsell(v.0.5.0), gtable(v.0.3.0), memoise(v.2.0.0), evaluate(v.0.14), labeling(v.0.4.2), knitr(v.1.37), fastmap(v.1.1.0), fansi(v.0.4.2), highr(v.0.9), Rcpp(v.1.0.7), backports(v.1.2.1), scales(v.1.1.1), cachem(v.1.0.5), jsonlite(v.1.7.2), farver(v.2.1.0), distill(v.1.3), digest(v.0.6.29), stringi(v.1.5.3), grid(v.4.0.5), cli(v.3.1.0), tools(v.4.0.5), magrittr(v.2.0.1), sass(v.0.4.0), tibble(v.3.1.0), crayon(v.1.4.1), pkgconfig(v.2.0.3), downlit(v.0.4.0), ellipsis(v.0.3.1), Matrix(v.1.3-2), assertthat(v.0.2.1), rmarkdown(v.2.11), rstudioapi(v.0.13), R6(v.2.5.0) and compiler(v.4.0.5)\n\n\n\n\n",
    "preview": "posts/2022-03-20-more-models/more-models_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-04-08T10:06:22-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-03-13-stacking-in-base-r/",
    "title": "Stacking vectors",
    "description": "Don't forget to use stack().",
    "author": [
      {
        "name": "Alice",
        "url": {}
      }
    ],
    "date": "2022-03-13",
    "categories": [],
    "contents": "\nIntro\nI have recently found a couple of great use cases for the stack() function from {utils}.\nBecause I want to remind my future self about this, I thought it would make a good short post to test this {distill} site that I just created!\nDocumentation\nFrom the stack() function documentation:\n\n“Stacking vectors concatenates multiple vectors into a single vector along with a factor indicating where each observation originated. Unstacking reverses this operation.”\n\nAn example\nSometimes, I get a bunch of vectors. Maybe I had multiple files or outputs with various items in them that correspond to different groups. Often, I need to combine these and then check how many of the items exist across multiple groups.\nFor the purpose of illustration, here I will pretend that I read into R a set of gene names as a named list.\n\n\nmy_list <- list(test1 = c(\"KRAS\",\"EGFR\",\"ERBB2\"),\n                test2 = c(\"ERBB2\",\"ERBB3\",\"SPRY2\",\"AR\"),\n                test3 = c(\"APC\",\"BRAF\"))\n\n\n\nstack() makes a nice tidy data.frame! (Note that this would also work if the input was a nested list of lists.)\n\n\nstack(my_list)\n\n\n  values   ind\n1   KRAS test1\n2   EGFR test1\n3  ERBB2 test1\n4  ERBB2 test2\n5  ERBB3 test2\n6  SPRY2 test2\n7     AR test2\n8    APC test3\n9   BRAF test3\n\nIf you table() the result from stack(), now you have a nice matrix of the values in each group.\n\n\ntable(stack(my_list))\n\n\n       ind\nvalues  test1 test2 test3\n  APC       0     0     1\n  AR        0     1     0\n  BRAF      0     0     1\n  EGFR      1     0     0\n  ERBB2     1     1     0\n  ERBB3     0     1     0\n  KRAS      1     0     0\n  SPRY2     0     1     0\n\nThe resulting object is a table. You can convert it to a data.frame.\n\n\nas.data.frame.array(table(stack(my_list)))\n\n\n      test1 test2 test3\nAPC       0     0     1\nAR        0     1     0\nBRAF      0     0     1\nEGFR      1     0     0\nERBB2     1     1     0\nERBB3     0     1     0\nKRAS      1     0     0\nSPRY2     0     1     0\n\nYou can also convert the binary matrix to logical (TRUE/FALSE).\n\n\ntable(stack(my_list)) > 0\n\n\n       ind\nvalues  test1 test2 test3\n  APC   FALSE FALSE  TRUE\n  AR    FALSE  TRUE FALSE\n  BRAF  FALSE FALSE  TRUE\n  EGFR   TRUE FALSE FALSE\n  ERBB2  TRUE  TRUE FALSE\n  ERBB3 FALSE  TRUE FALSE\n  KRAS   TRUE FALSE FALSE\n  SPRY2 FALSE  TRUE FALSE\n\nNow, imagine a case where you have the table and some values are greater than 1 (because they appeared in a list more than once). You can use a trick to convert to logical and back to numeric 0/1.\n\n\nmy_list_w_repeats <- list(\n  test1 = c(\"KRAS\",\"EGFR\",\"ERBB2\"),\n  test2 = c(\"ERBB2\",\"ERBB3\",\"SPRY2\",\"AR\"),\n  test3 = c(\"APC\",\"APC\",\"APC\",\"BRAF\")) # APC is here 3 times\n\ntable(stack(my_list_w_repeats))\n\n\n       ind\nvalues  test1 test2 test3\n  APC       0     0     3\n  AR        0     1     0\n  BRAF      0     0     1\n  EGFR      1     0     0\n  ERBB2     1     1     0\n  ERBB3     0     1     0\n  KRAS      1     0     0\n  SPRY2     0     1     0\n\n+(table(stack(my_list_w_repeats)) > 0)\n\n\n       ind\nvalues  test1 test2 test3\n  APC       0     0     1\n  AR        0     1     0\n  BRAF      0     0     1\n  EGFR      1     0     0\n  ERBB2     1     1     0\n  ERBB3     0     1     0\n  KRAS      1     0     0\n  SPRY2     0     1     0\n\nSummary\nI forget about this function every once in a while and it is really useful. I also have a gist about this.\nFor fun, here is one way to do this with {dplyr} and {tidyr}. I would like to hear about other ways because I don’t find this as intuitive.\n\n\nlibrary(dplyr, quietly = TRUE)\n\nlapply(my_list, function(x) data.frame(genes = x)) %>% \n  bind_rows(.id = \"names\")\n\n\n  names genes\n1 test1  KRAS\n2 test1  EGFR\n3 test1 ERBB2\n4 test2 ERBB2\n5 test2 ERBB3\n6 test2 SPRY2\n7 test2    AR\n8 test3   APC\n9 test3  BRAF\n\nNow to make the binary matrix.\n\n\nlapply(my_list, function(x) data.frame(genes = x)) %>% \n  bind_rows(.id = \"names\") %>%\n  count(names, genes) %>%\n  tidyr::pivot_wider(names_from = \"names\",\n                     values_from = \"n\",\n                     values_fill = 0)\n\n\n# A tibble: 8 x 4\n  genes test1 test2 test3\n  <chr> <int> <int> <int>\n1 EGFR      1     0     0\n2 ERBB2     1     1     0\n3 KRAS      1     0     0\n4 AR        0     1     0\n5 ERBB3     0     1     0\n6 SPRY2     0     1     0\n7 APC       0     0     1\n8 BRAF      0     0     1\n\nsessionInfo\n\n\nsessionInfo()\n\n\nR version 4.0.5 (2021-03-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n[1] dplyr_1.0.5\n\nloaded via a namespace (and not attached):\n [1] rstudioapi_0.13   knitr_1.37        magrittr_2.0.1   \n [4] tidyselect_1.1.0  downlit_0.4.0     R6_2.5.0         \n [7] rlang_0.4.10      fastmap_1.1.0     fansi_0.4.2      \n[10] stringr_1.4.0     tools_4.0.5       xfun_0.30        \n[13] utf8_1.2.1        cli_3.1.0         DBI_1.1.1        \n[16] jquerylib_0.1.4   htmltools_0.5.1.1 ellipsis_0.3.1   \n[19] assertthat_0.2.1  yaml_2.2.1        digest_0.6.29    \n[22] tibble_3.1.0      lifecycle_1.0.0   crayon_1.4.1     \n[25] tidyr_1.1.3       purrr_0.3.4       sass_0.4.0       \n[28] vctrs_0.3.7       distill_1.3       memoise_2.0.0    \n[31] glue_1.4.2        cachem_1.0.5      evaluate_0.14    \n[34] rmarkdown_2.11    stringi_1.5.3     compiler_4.0.5   \n[37] bslib_0.2.5.1     pillar_1.6.0      generics_0.1.0   \n[40] jsonlite_1.7.2    pkgconfig_2.0.3  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-21T12:15:07-04:00",
    "input_file": {}
  }
]
