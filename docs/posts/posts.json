[
  {
    "path": "posts/2024-08-01-what-i-am-reading-q2-2024/",
    "title": "What I am reading: Q2 2024",
    "description": "Summer reading.",
    "author": [
      {
        "name": "Alice Walsh",
        "url": {}
      }
    ],
    "date": "2024-08-01",
    "categories": [],
    "contents": "\nHere are articles or newsletters that have gone around in Q2 2024 that I have read or found interesting. But, I would love some new recommendations!\nSee my previous post for more general newsletters that I follow.\nSingle posts or articles that were interesting:\nMusings on Building a Generative AI Product (LinkedIn)\nLAB-Bench: Measuring Capabilities of Language Models for Biology Research (FutureHouse)\nDSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines (preprint)\nAssessing the limits of zero-shot foundation models in single-cell biology (preprint)\nWhat are the best long form lectures on YT? (X/twitter thread)\nAI models collapse when trained on recursively generated data\nsessionInfo\n\n\npander::pander(sessionInfo())\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nlocale:\nen_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages:\nstats, graphics, grDevices, utils, datasets, methods and base\nloaded via a namespace (and not attached):\ndigest(v.0.6.31), R6(v.2.5.1), fastmap(v.1.1.1), xfun(v.0.39), cachem(v.1.0.7), knitr(v.1.42), distill(v.1.6), memoise(v.2.0.1), htmltools(v.0.5.5), rmarkdown(v.2.21), cli(v.3.6.1), pander(v.0.6.5), downlit(v.0.4.2), sass(v.0.4.5), jquerylib(v.0.1.4), withr(v.2.5.0), compiler(v.4.3.0), rstudioapi(v.0.15.0), tools(v.4.3.0), bslib(v.0.4.2), evaluate(v.0.20), Rcpp(v.1.0.10), yaml(v.2.3.7), jsonlite(v.1.8.4) and rlang(v.1.1.1)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2024-08-01T12:28:30-04:00",
    "input_file": "what-i-am-reading-q2-2024.knit.md"
  },
  {
    "path": "posts/2024-04-26-what-i-am-reading-q1-2024/",
    "title": "What I am reading: Q1 2024",
    "description": "There is too much content out there.",
    "author": [
      {
        "name": "Alice Walsh",
        "url": {}
      }
    ],
    "date": "2024-04-26",
    "categories": [],
    "contents": "\nKeeping up with biotech, tech, and techbio is exhausting. It seems like every other person has a substack.\nHere are articles or newsletters that have gone around in Q1 2024 that I have found valuable. I would love some new recommendations!\nGeneral newsletters or blogs:\nBitsinbio newsletter: https://bitsinbio.substack.com/\nAbigail Haddad’s substack on LLMs: https://presentofcoding.substack.com/\nVicki Boykis is always a good read: https://vickiboykis.com/\nDecoding bio is from a group of investors: https://www.decodingbio.com/\nWaseem Daher, startup real talk: https://waseem.substack.com/\nSingle posts or articles that were interesting:\nSelling AI Products and Services to Big Pharma (Brandon White)\nRules of ML (Martin Zinkevich):\nProblem choice and decision trees in science and engineering (note: the right questions are the hard - part, not the right solutions)\nAn Incomplete History of Selventa and the Biological Expression Language (BEL) (Charles Tapley - Hoyt)\nI don’t know how I found this, but I remember Selventa, and this is fascinating.\n\nSome questions about biotech that I find interesting (Alex Telford)\nA deep profile of gene expression across 18 human cancers (Wei Qiu et al.)\nI like latent variables\n\nNo installation required: how WebAssembly is changing scientific computing (By Jeffrey M. Perkel)\nsessionInfo\n\n\npander::pander(sessionInfo())\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nlocale:\nen_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages:\nstats, graphics, grDevices, utils, datasets, methods and base\nloaded via a namespace (and not attached):\ndigest(v.0.6.31), R6(v.2.5.1), fastmap(v.1.1.1), xfun(v.0.39), cachem(v.1.0.7), knitr(v.1.42), distill(v.1.6), memoise(v.2.0.1), htmltools(v.0.5.5), rmarkdown(v.2.21), cli(v.3.6.1), pander(v.0.6.5), downlit(v.0.4.2), sass(v.0.4.5), jquerylib(v.0.1.4), withr(v.2.5.0), compiler(v.4.3.0), rstudioapi(v.0.15.0), tools(v.4.3.0), bslib(v.0.4.2), evaluate(v.0.20), Rcpp(v.1.0.10), yaml(v.2.3.7), jsonlite(v.1.8.4) and rlang(v.1.1.1)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2024-04-26T09:59:28-04:00",
    "input_file": "what-i-am-reading-q1-2024.knit.md"
  },
  {
    "path": "posts/2024-03-26-improving-your-job-search/",
    "title": "Improving Your Job Search",
    "description": "My common advice for people looking for comp bio jobs.",
    "author": [
      {
        "name": "Alice Walsh",
        "url": {}
      }
    ],
    "date": "2024-03-26",
    "categories": [],
    "contents": "\nRecently, I have been talking with some job seekers and interviewing for a couple of positions on my team. I was reminded how difficult it is for job seekers to get noticed and, on the flip side, for hiring managers to identify relevant candidates from a pile of candidates. I wanted to write down a few ideas in case they could be helpful.\nHow to apply\nLiberally. Interviews are good practice. You should not waste people’s time if you have no intention of accepting a new role, but if you are seeking a job, don’t count yourself out by not even applying.\nIntroductions and talking to people you know at a company is absolutely a good idea. I have heard the cynical view that no one gets hired that was not referred. That has not been my experience. At least at established companies, most people hired are not friends or connections. However, the current climate (March 2024) is tough for applicants. Companies are getting way more candidates than they could review. Any help you can get to get your resume even seen seems worth the effort.\nResumés\nThere is already more advice than you could use on the internet about writing a good resume. Here is my approach.\nI like to see what people have accomplished (results). Writing clear actions you took in a past role is more exciting than a job title, location, or time frame.\nPeople who bring their experience to life in their resumes and cover letters stand out. Add some details that will be interesting and specific to you.\nPlease include links to GitHub or other places where I can see your work. If you don’t have a GitHub or LinkedIn account, that is a red flag.\nI can often tell when a resume is customized for a job - do this! You can highlight relevant experience for the job, and it is an opportunity to differentiate yourself.\nI like cover letters. If there are many applicants, one of the easiest ways to differentiate yourself is by personalizing your message to the application and demonstrating genuine interest.\nA “skills” section has little value to me. It all looks the same and doesn’t tell me anything meaningful, so part of the interview process has to involve a deeper dive into past experience.\nWhen to look for a job\nWhile employers often have flexibility, it is best to be in active interviews when you can start the role within six weeks. A hiring manager would like the new hire to begin as soon as possible. A complete interview process, from reading your CV to extending an offer, could be as short as two weeks or as long as six weeks.\nThis advice is most salient to people completing full-time degrees. I see many applications from people who will not graduate for several months. I completely understand the need to look for a job while completing your degree, but the timeline has to make sense.\nThe timeline gets fuzzier if you are more established in your career. You should network and strategize to find your next role, which can take a long time (people say one to two years) to build connections and learn about possible future opportunities.\nHow to interview\nBe prepared! I cannot emphasize enough that the interview process is often less about how you answer our questions and more about how you ask us questions. I frequently hear that hiring managers are looking for curiosity, passion, and potential. Having good questions about the company and the role can set you apart. Here is my advice: Ask people you have worked with, or you know who hire people about the best questions they have been asked by candidates. Ask your favorite chat LLM for inspiration, too.\nThe other recommendation I have for candidates is about responsiveness. If at all possible, do not delay replying to emails about scheduling. I will read this as a passive “no thanks.” It’s fine, but I prefer a polite decline rather than no response. If you will be traveling or unresponsive, send a note to that effect. It is extremely obvious how interested a candidate is based on how quickly they reply to schedule interviews or answer questions.\nOn a related note, fewer and fewer people send thank-you notes after interviews, so I do not consider that when forming an opinion on a candidate. I don’t really even notice anymore. If I were interviewing, I would send a thank you. I don’t see any downside, so why not?\nDisclaimers and an offer\nA caveat: My work has been entirely in the life sciences, biotech, and pharma space, and I don’t know how generalizable this perspective is.\nIn the past decade, I have participated in many interviews at large and small companies (easily in the hundreds). Feel free to reach out if you are applying for roles in biotech and pharma and are interested in talking about it. I have set aside some time to talk about these sorts of issues. I would be happy to provide resume advice or answer your questions about interviewing.\nsessionInfo\n\n\npander::pander(sessionInfo())\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nlocale:\nen_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages:\nstats, graphics, grDevices, utils, datasets, methods and base\nloaded via a namespace (and not attached):\ndigest(v.0.6.31), R6(v.2.5.1), fastmap(v.1.1.1), xfun(v.0.39), cachem(v.1.0.7), knitr(v.1.42), distill(v.1.6), memoise(v.2.0.1), htmltools(v.0.5.5), rmarkdown(v.2.21), cli(v.3.6.1), pander(v.0.6.5), downlit(v.0.4.2), sass(v.0.4.5), jquerylib(v.0.1.4), withr(v.2.5.0), compiler(v.4.3.0), rstudioapi(v.0.15.0), tools(v.4.3.0), bslib(v.0.4.2), evaluate(v.0.20), Rcpp(v.1.0.10), yaml(v.2.3.7), jsonlite(v.1.8.4) and rlang(v.1.1.1)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2024-03-26T07:56:38-04:00",
    "input_file": "improving-your-job-search.knit.md"
  },
  {
    "path": "posts/2023-12-03-job-report-2023/",
    "title": "Data Science Jobs in Pharma in 2023",
    "description": "The job market is tough right now.",
    "author": [
      {
        "name": "Alice Walsh",
        "url": {}
      }
    ],
    "date": "2023-12-03",
    "categories": [],
    "contents": "\nAs many people know too well, it has not been a good year for jobs in pharma and biotech. As STAT News reported, it is hard out there for job seekers.\nI have previously done some data mining for pharma job postings, so I went ahead and browsed more recent data to understand whether data science jobs might have been affected more or less than other roles.\nMy methods are imperfect, but I focused on top pharma and jobs descriptions that focused on data science and related-skills.\nOverall, jobs are down\nI used data from November 2022, February 2023, and November 2023 (when I downloaded the job descriptions).\n\n\nShow code\n\nlibrary(dplyr)     # data wrangling\nlibrary(gt)        # make nice tables\nlibrary(ggplot2)   # nice plots\n\n# plot theme\ntheme_set(theme_minimal(base_family = \"Avenir\"))\n# set seed\nset.seed(1203)\n\ndata_files <- c(\"2022-11-03/combined_data2.csv\",\n                \"2023-02-20/combined_data2.csv\",\n                \"2023-11-21/combined_data2.csv\")\njobs <- purrr::map_dfr(\n  file.path(\"~/Documents/code/job_description_collector/data/\", data_files), \n  read.csv, .id = \"id\")\ndate_map <- data.frame(\n  id = as.character(1:length(data_files)),\n  date = dirname(data_files)\n)\njobs <- jobs |> \n  left_join(date_map, by = \"id\")\n\n# require distinct descriptions\njobs <- distinct(jobs,\n                 date, company, title, description, .keep_all = TRUE) |> \n  add_count(date, company, name = \"n_company\")\n\n\nLooking at all jobs, we can see that the postings have decreased from February to November.\n\n\nShow code\n\njobs |> \n  distinct(date, n_company, company) |> \n  ggplot(aes(date, n_company, group = company)) + \n  geom_line() +\n  geom_point(aes(color = company)) + \n  coord_cartesian(ylim = c(0, NA)) + \n  labs(title = \"Total jobs per company\",\n       y = \"Jobs\", x = \"Date\")\n\n\n\nThere are few data science jobs\nNext, I labeled jobs as “data jobs” if the descriptions had R or data science in them.\nMost the jobs are not “data jobs,” so to look at the trends, I normalize the values to the first data point (November, 2022).\nBack in November of 2022, there were 232 jobs that mentioned R or “data science”. In November of 2023, that is down to only 66 jobs.\nI also plotted “chemistry” and “sales” as comparisons. It seems that some types of roles have been hit harder than others!\n\n\nShow code\n\njobs_w_type <- mutate(jobs, \n               job_type = case_when(\n                 grepl(\"[^(Maurice)] R[\\\\., ][^(&N.Ph)]\", description) |\n                   grepl(\"[Dd]ata [Ss]science\", description) ~ \"data\",\n                 grepl(\"[Cc]hemistry\", description) ~ \"chemistry\",\n                 grepl(\"[Ss]ales\", description) ~ \"sales\",\n                 TRUE ~ \"other\")\n) |> \n  count(job_type, date) |> \n  group_by(job_type) |> \n  mutate(fraction = n / n[date == \"2022-11-03\"]) \n\njobs_w_type |> \n  ggplot(aes(date, fraction, group = job_type, color = job_type)) + \n  geom_line() +\n  geom_point() + \n  coord_cartesian(ylim = c(0, NA)) + \n  scale_y_continuous(labels = scales::percent) + \n  scale_color_manual(values = c(\"#87B8DA\", \"firebrick4\", \"#87B8DA\", \"#87B8DA\")) + \n  geom_text(aes(label = job_type), nudge_x = 0.1, hjust = 0,\n            data = filter(jobs_w_type, date == \"2023-11-21\"),\n            family = \"Avenir\") + \n  labs(title = \"Jobs by type\",\n       y = NULL, x = \"Date\",\n       color = \"Job type\") + \n  theme(legend.position = \"none\")\n\n\n\nGood luck out there job seekers! If anyone is looking for job search advice, feel free to reach out to see if I can help.\nsessionInfo\n\n\npander::pander(sessionInfo())\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nlocale:\nen_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages:\nstats, graphics, grDevices, utils, datasets, methods and base\nother attached packages:\nggplot2(v.3.4.4), gt(v.0.10.0) and dplyr(v.1.1.2)\nloaded via a namespace (and not attached):\ngtable(v.0.3.3), jsonlite(v.1.8.4), highr(v.0.10), compiler(v.4.3.0), Rcpp(v.1.0.10), tidyselect(v.1.2.0), xml2(v.1.3.4), jquerylib(v.0.1.4), scales(v.1.2.1), yaml(v.2.3.7), fastmap(v.1.1.1), R6(v.2.5.1), labeling(v.0.4.2), generics(v.0.1.3), knitr(v.1.42), tibble(v.3.2.1), pander(v.0.6.5), distill(v.1.6), munsell(v.0.5.0), bslib(v.0.4.2), pillar(v.1.9.0), rlang(v.1.1.1), utf8(v.1.2.3), cachem(v.1.0.7), xfun(v.0.39), sass(v.0.4.5), memoise(v.2.0.1), cli(v.3.6.1), withr(v.2.5.0), magrittr(v.2.0.3), digest(v.0.6.31), grid(v.4.3.0), rstudioapi(v.0.15.0), lifecycle(v.1.0.3), vctrs(v.0.6.4), downlit(v.0.4.2), evaluate(v.0.20), glue(v.1.6.2), farver(v.2.1.1), fansi(v.1.0.4), colorspace(v.2.1-0), purrr(v.1.0.2), rmarkdown(v.2.21), tools(v.4.3.0), pkgconfig(v.2.0.3) and htmltools(v.0.5.5)\n\n\n\n\n",
    "preview": "posts/2023-12-03-job-report-2023/job-report-2023_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2023-12-03T18:42:34-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-11-09-rpharma2022/",
    "title": "R in Pharma Conference Talk 2022",
    "description": "I co-presented at R/Pharma 2022 with Katie Igartua.",
    "author": [
      {
        "name": "Alice Walsh",
        "url": {}
      }
    ],
    "date": "2022-11-09",
    "categories": [],
    "contents": "\nKatie Igartua, PhD (Sr Director, Computational Biology at Tempus) and\nI presented the opening keynote at R/Pharma 2022 on October 8th.\nThanks to the organizers for having me and making this a very\npositive experience! Double thanks to Katie for agreeing to take this on\nwith me and then brainstorming a (hopefully) interesting topic.\n\n\nHere is a link to the slides\nIf you want to get in touch to talk more about ideas to engage to\nhelp people get hired and find good people in the R/Pharma community, do\nreach out. The best way to get in touch at the moment is probably LinkedIn.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-01-19T07:20:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-07-pharma-data-jobs/",
    "title": "Data Jobs in Pharma",
    "description": "Who is hiring data scientists? I mined some job descriptions to look for trends.",
    "author": [
      {
        "name": "Alice Walsh",
        "url": {}
      }
    ],
    "date": "2022-10-07",
    "categories": [],
    "contents": "\n\nContents\nBackground\nReview the\ndata\nWho is hiring for specific\nskills?\nWhat titles should we be\nlooking for?\nWhat’s next?\n\nContinuing with the theme of my last few posts about pharma data, I\nwas interested in exploring trends in job descriptions across different\nbig pharma companies.\nThis rainy-day project might interest other data folks in the pharma\nindustry. Pharma companies are hiring for many roles that need R/Python\nskills. I am interested in how these teams can be successful across the\npharma ecosystem and translate the investment in data science into\nmeaningful gains that can lead to better medicines.\nBackground\nI was primarily interested in the following:\nHow many data-intensive jobs (which I consider a catch-all term for\ndata science, biostatistics, computational biology, data engineering,\netc.) are major pharma companies hiring for?\nAre some companies hiring for more “data jobs” than others?\nAre there differences in the skills these companies are looking\nfor?\nI collected data from a popular job posting site using web scraping\ntechniques. I used python and bash scripts to do that part. I will focus\nhere on some exploratory analysis and observations.\nI used the same list of the largest pharma companies as my previous\npost on data science tweets (and added Genentech because their posts\nare separate from Roche).\nReview the data\nFirst, load some packages.\n\n\nlibrary(dplyr)     # data wrangling\nlibrary(gt)        # make nice tables\nlibrary(ggplot2)   # nice plots\nlibrary(tidytext)  # used for tokenizing bigrams\nlibrary(tidylo)    # used to compare word usage between posts\n\n# plot theme\ntheme_set(theme_minimal(base_family = \"Avenir\"))\ncolor_values <-\n  setNames(colorspace::desaturate(\n    c(\"deepskyblue1\",\n      \"deepskyblue2\",\n      \"deepskyblue3\",\n      \"deepskyblue4\"), amount = 0.5),\n    as.factor(1:4))\n\n# set seed\nset.seed(1786)\n\n\n\nThe data contains information on the company, title, and the full\ntext of the description. The data is limited to jobs located in the\nUnited States.\n\n\njobs <- read.csv(\"~/Documents/code/pharma_job_analysis/data/2022-09-28/combined_data.csv\")\nglimpse(jobs)\n\n\nRows: 8,210\nColumns: 4\n$ title       <chr> \"Associate Scientist I\", \"Lab Technician\", \"QC S…\n$ company     <chr> \"ABBVIE\", \"ABBVIE\", \"ABBVIE\", \"ABBVIE\", \"ABBVIE\"…\n$ link        <chr> \"https://www.indeed.com/rc/clk?jk=e5833a40be5101…\n$ description <chr> \"Completely follow established experimental prot…\n\nThe number of job postings varied by company, and I de-duplicated to\nremove posts that had identical titles and descriptions. These could be\nmultiple openings for the same role or duplicate postings. After some\nexperimentation, I decided to remove the duplicates. This decision had a\nlarger impact on some companies than others. For example, GSK had about\n40% duplicates in the original dataset, and I might be underestimating\nopen roles there.\n\n\njobs <- distinct(jobs, company, title, description, .keep_all = TRUE) %>%\n  add_count(company, name = \"n_company\") %>%\n  filter(n_company > 30)\n\ncount(jobs, company, sort = T) %>%\n  gt(caption = \"The number of job postings by company\") %>%\n  tab_options(data_row.padding = px(1))\n\n\n\nTable 1: The number of job postings by company\n  \n  company\n      n\n    Merck\n867Pfizer\n737ABBVIE\n726Bristol Myers Squibb\n678AstraZeneca\n653Sanofi\n632GSK\n572Novartis\n465Genentech\n434Takeda Pharmaceutical\n302Roche\n274Johnson & Johnson\n263Eli Lilly\n247\n\nWho is hiring for specific\nskills?\nTo find data-intensive jobs, I quantified the number of postings by\ncompany that contained specific keywords in the description. Most job\ndescriptions have a section that lists the various skills/experience\ndesired for the job. Searching for “R,” “Python,” or “SAS” could help me\nidentify the types of jobs I am interested in, independent of the title\nor the level of the job. The description lengths varied widely by\ncompany. Abbvie postings were the shortest, with a median of 446 words,\nwhile GSK postings were the longest, with a median of 1272 words.\nI wrote a helper function to summarize the number of jobs with each\nkeyword and adjusted the input regex to get the best results. I\npresented the results in a {ggplot2} annotated bar chart. However, this\ninformation can be summarized nicely in a table with packages like {reactable}.\nI also wrote a function (not shown) to return the text in the\ndescription surrounding the keyword so that I could do some quality\ncontrol and ensure that I was only getting the desired results. For\nexample, if I wanted all jobs that contain “AI,” I need to be careful\nbecause I am not interested in jobs related to “AIDS” or “PAI.”\n\n\n#' Summarize job postings by keyword\n#'\n#' @param data jobs data.frame with description, company\n#' @param pattern a regex to search for within description\n#'\n#' @return a tibble with number of jobs negative, positive,\n#' and percent positive for the keyword by company\ndescription_search <- function(data, pattern) {\n  count(data, company, search = grepl(pattern, description)) %>%\n    tidyr::pivot_wider(names_from = search,\n                       values_from = n, values_fill = 0) %>%\n    rename(\"pos\" = `TRUE`, \"neg\" = `FALSE`) %>%\n    mutate(percent = pos / (pos + neg)) %>%\n    arrange(desc(percent))\n}\n\nplot_description_search <- function(data,\n                                 pattern,\n                                 title = NULL) {\n  gplot <- description_search(data, pattern) %>%\n    mutate(company = forcats::fct_reorder(company, percent)) %>%\n    mutate(color = cut(x = percent, breaks = c(0, 0.05, 0.1, 0.25, 1),\n                       include.lowest = T),\n           color = as.factor(as.numeric(color))) %>%\n    mutate(n_label = glue::glue(\"{ pos } / { neg + pos }\"),\n           p_label = paste(round(100 * percent), \"%\")) %>%\n    ggplot(aes(y = percent, x = company)) +\n    geom_col(aes(fill = color), show.legend = FALSE) +\n    geom_label(aes(label = p_label,\n                   y = percent - (max(percent) * 0.02)),\n               size = 3, family = \"Avenir\", hjust = 1) +\n    geom_text(aes(label = n_label, y = percent + (max(percent) * 0.02)),\n              size = 3, family = \"Avenir\", hjust = 0) +\n    scale_fill_manual(values = color_values) +\n    scale_y_continuous(labels = scales::percent,\n                       expand = expansion(mult = c(0, .18))) +\n    coord_flip() +\n    labs(x = NULL, y = NULL,\n         title = title) +\n    theme(plot.title.position = \"plot\",\n          panel.grid.major.y = element_blank(),\n          axis.text.x = element_blank(),\n          axis.text.y = element_text(hjust = 1, margin = margin(r = 0)))\n  gplot\n}\n\n\n\nLet’s look at results for some common programming languages.\n\n\nplot_description_search(jobs, \"[Pp]ython\",\n                        title = \"How many jobs include 'Python'?\")\n\n\n\nplot_description_search(jobs, \"[^(Maurice)] R[\\\\., ][^(&N.Ph)]\",\n                        title = \"How many jobs include 'R'?\")\n\n\n\nplot_description_search(jobs, \"SAS\", title = \"How many jobs include 'SAS'?\")\n\n\n\n\nIt looks like Merck was doing a lot of hiring for data-intensive\nroles when I collected this data.\nWhat titles should we be\nlooking for?\nI was also interested in the titles for the jobs that required\nR/Python/SAS/etc.\nJob titles are not standardized, so counting each exact title’s\noccurrence was not informative. To illustrate, here is the count of\nunique job titles that mention ‘R’ in the description.\n\n\nr_jobs <- jobs %>%\n  filter(grepl(\"[^(Maurice)] R[\\\\., ][^(&N.Ph)]\", description))\n\nn_distinct(r_jobs$title)\n\n\n[1] 250\n\nTherefore, I decided to tokenize the titles and consider bigrams (a\npair of consecutive words) using the {tidytext}\npackage.\n\n\ndata_jobs_titles <- jobs %>%\n  mutate(r = grepl(\"[^(Maurice)] R[\\\\., ][^(&N.Ph)]\", description)) %>%\n  mutate(title = stringr::str_remove_all(title, \"and\")) %>%\n  unnest_tokens(word, title, token = \"ngrams\", n = 2)\n\n\n\nBelow is a table with bigrams more likely to appear in jobs with “R”\nin the description. The results are similar for Python and SAS (many\njobs listed all these skills). I used the {tidylo} package to\ncalculate these statistics.\n\n\ndata_jobs_titles %>%\n  count(r, word) %>%\n  bind_log_odds(r, word, n, uninformative = FALSE) %>%\n  arrange(desc(log_odds_weighted)) %>%\n  slice(1:15) %>%\n  select(word, n, log_odds_weighted) %>%\n  gt() %>%\n  tab_options(data_row.padding = px(1))\n\n\n\nword\n      n\n      log_odds_weighted\n    associate director\n51\n24.624656senior scientist\n24\n18.718214principal scientist\n27\n18.574961associate scientist\n3\n13.341991data scientist\n30\n11.839345pharmacology pharmacometrics\n28\n11.739631senior manager\n5\n11.102193co op\n5\n10.567063r d\n6\n10.504087quantitative pharmacology\n22\n10.398723associate principal\n10\n10.030570cell therapy\n4\n9.976590director clinical\n11\n9.590808senior principal\n13\n9.488367postdoctoral fellow\n6\n9.296038\n\nMany jobs that require R are for more senior roles like “associate\ndirector” or “principal scientist.” Terms related to the department or\nfunction, like “pharmacology pharmacometrics” and “r d,” also rank\nhighly. This result is consistent with these jobs requiring advanced\ndegrees and training.\nWhat’s next?\nInitially, I was surprised that the percentage of data-related job\npostings was so high, considering all the types of roles at a big pharma\ncompany (Johnson & Johnson has over 140,000 employees)!\nThat said, this is consistent with my hypothesis that data-intensive\njobs are in demand and that companies are working hard to hire more data\nprofessionals and retain them.\nThis was a side project, and I did not invest time finding multiple\ndata sources or testing different methodologies. A major shortcoming is\nthat I don’t have longitudinal data. I grabbed data on a single day and\ncannot look at trends over time, which would be interesting. Maybe I\nwill do a follow-up analysis in the future with updated data. Given that\nmy data was a snapshot from September 2022, I am skeptical of drawing\ntoo many conclusions about comparing individual companies.\nsessionInfo\n\n\npander::pander(sessionInfo())\n\n\nR version 4.0.5 (2021-03-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale:\nen_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats,\ngraphics, grDevices, utils,\ndatasets, methods and base\nother attached packages: tidylo(v.0.2.0),\ntidytext(v.0.3.1), ggplot2(v.3.3.6),\ngt(v.0.7.0) and dplyr(v.1.0.9)\nloaded via a namespace (and not attached):\ntidyselect(v.1.1.2), xfun(v.0.31),\nbslib(v.0.2.5.1), pander(v.0.6.3),\npurrr(v.0.3.4), lattice(v.0.20-41),\ncolorspace(v.2.0-3), vctrs(v.0.4.1),\ngenerics(v.0.1.3), htmltools(v.0.5.3),\nSnowballC(v.0.7.0), yaml(v.2.3.5),\nutf8(v.1.2.2), rlang(v.1.0.4),\njquerylib(v.0.1.4), pillar(v.1.8.0),\nglue(v.1.6.2), withr(v.2.5.0), DBI(v.1.1.1),\nlifecycle(v.1.0.1), stringr(v.1.4.0),\nmunsell(v.0.5.0), gtable(v.0.3.0),\nmemoise(v.2.0.0), evaluate(v.0.15),\nlabeling(v.0.4.2), knitr(v.1.39),\nforcats(v.0.5.1), fastmap(v.1.1.0),\nfansi(v.1.0.3), highr(v.0.9),\ntokenizers(v.0.2.1), Rcpp(v.1.0.9),\nscales(v.1.2.0), cachem(v.1.0.5),\njsonlite(v.1.8.0), farver(v.2.1.1),\ndistill(v.1.3), digest(v.0.6.29),\nstringi(v.1.7.8), grid(v.4.0.5),\ncli(v.3.3.0), tools(v.4.0.5),\nmagrittr(v.2.0.3), sass(v.0.4.1),\ntibble(v.3.1.8), janeaustenr(v.0.1.5),\ncrayon(v.1.5.1), tidyr(v.1.2.0),\npkgconfig(v.2.0.3), downlit(v.0.4.0),\nellipsis(v.0.3.2), Matrix(v.1.3-2),\nassertthat(v.0.2.1), rmarkdown(v.2.11),\nrstudioapi(v.0.13), R6(v.2.5.1) and\ncompiler(v.4.0.5)\n\n\n\n\n",
    "preview": "posts/2022-10-07-pharma-data-jobs/pharma-data-jobs_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-10-07T17:55:52-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-10-02-replotting-clinical-trials/",
    "title": "Clinical Trial Visualizations",
    "description": "Re-analysis of publicly-available clinical trial data",
    "author": [
      {
        "name": "Alice Walsh",
        "url": {}
      }
    ],
    "date": "2022-10-02",
    "categories": [],
    "contents": "\nI work in drug development, where we often discuss two things:\nIt takes too long to develop drugs\nIt costs too much to develop drugs (and it is getting more\nexpensive)\nThis observation is sometimes referred to as Eroom’s law\n(Moore’s law backward).\nIn this post, I look at clinical trial data and make a couple of\nvisualizations.\n\nThis post is mainly for future me to remind myself how to work with\nclinicaltrials.gov data and some {ggplot2} code that I often forget\n(annotating and customizing plots).\nBackground\nMany researchers have studied drug development trends and examined\npossible approaches to increase the probability of success and decrease\nthe time and costs of bringing a good drug to market.\nFor example:\nThis\nreport on 2006-2015 from BIO, Informa, Amplion\n\nThe overall likelihood of approval (LOA) from Phase I for all\ndevelopmental candidates was 9.6%, and 11.9% for all indications outside\nof Oncology.\n\nThis\nupdate for 2011-2020\n\nThe overall likelihood of approval (LOA) from Phase I for all\ndevelopmental candidates over 2011–2020 was 7.9%. On average, it takes\n10.5 years for a Phase I asset to progress to regulatory approval.\nDisease areas with above-average LOAs tend to have the shortest\ndevelopment timelines.\n\nSeveral publications have shown that targets with human genetic\nevidence are more likely to be successful (Ochoa et al, 2022,\nKing et al,\n2019, Nelson et al.,\n2015)\nPublications have also shown that drugs with patient selection\nbiomarkers are more likely to be successful (Wong et al,\n2019)\nWhat data is available?\nMost of the papers and reports cited above use data from proprietary\ndatabases that are not available for free. Or they performed some\nchallenging (or even heroic) data integration.\nI wanted to explore what was publicly available and at least\nre-create the plots in those reports.\nHow long does each\nphase of development take?\nI will use data from clinicaltrials.gov. There are\nmany ways to interact with this data, such as the API. Some R\npackages exist to work with this data in R, but I have not tested\nthem.\nHere, I downloaded a snapshot of the data from the AACT\ndatabase, which provides a relational database that downloads data\nfrom clinicaltrials.gov daily. These snapshots are relatively large\n(~1.4 GB).\nLoad packages\n\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\n\n# set up plot preferences\ntheme_set(theme_minimal(base_family = \"Avenir Next\"))\npalette_colors <- c(\"#c1e7ff\", \"#6996b3\", \"#004c6d\")\n\n\n\nLoad clinicaltrials.gov data\nI loaded a small set of tables (out of 50 total) that I need from the\n2022-08-30 snapshot as a list of data.frames in R.\n\n\npath_to_data <- \"~/AACT/20220830/\"\ntables_to_read <- rlang::set_names(c(\"studies\", \"interventions\", \"conditions\",\n                                     \"sponsors\", \"designs\"))\ndb <- lapply(tables_to_read, \n             function(table) read_delim(paste0(path_to_data, table, \".txt\"),\n                                        delim = \"|\"))\n\n\n\nFilter to the desired studies\nThe data contained information on over 400,000 studies with the\nearliest entries dating to 1999.\nThe code below filters to the studies I am most interested in.\nOnly used “Completed” or “Terminated” studies\nRestricted to completion dates between 2000-01-01 and\n2019-12-31\nRemoved trials that started and ended on the same day\nRestricted to Phase 1, 2, or 3 (removing things like “Early Phase 1”\nand many “Not Applicable”/NA. This also removes terms like\n“Phase1/Phase2”, which we may want to keep)\nRestricted to “Drug” or “Biological” intervention types (removing\ndevice, behavioral, etc.)\nRestricted to studies where the lead sponsor was industry (not\nacademic or government)\nRestricted to studies that list the primary purpose as “Treatment”\nor “Prevention” (not basic science, diagnostic, or other)\nMany other filters might be considered. Some of these trials might\nnot be very informative if we are interested only in novel drug\ndevelopment. For example, some of these trials test a new formulation or\ndelivery method.\n\n\nShow code\n\nselected_trials <- \n  db$studies %>% \n  filter(overall_status %in% c(\"Completed\", \"Terminated\"),\n         phase %in% c(\"Phase 1\", \"Phase 2\", \"Phase 3\"),\n         study_type == \"Interventional\",\n         completion_date >= as.Date(\"2000-01-01\"),\n         completion_date <= as.Date(\"2019-12-31\")) %>% \n  select(nct_id, phase, number_of_arms, source, enrollment,\n         overall_status, study_type, \n         completion_date, completion_date_type,\n         start_date, start_date_type,\n         brief_title, official_title,\n         results_first_posted_date, study_first_posted_date) %>% \n  semi_join({db$interventions %>%\n      filter(intervention_type %in% c(\"Drug\", \"Biological\"))},\n      by = \"nct_id\") %>%\n  inner_join({db$conditions %>% \n      tidyr::nest(conds = c(-nct_id))},\n      by = \"nct_id\") %>% \n  semi_join({db$sponsors %>%\n      filter(lead_or_collaborator == \"lead\" & agency_class == \"INDUSTRY\")},\n      by = \"nct_id\") %>%\n  inner_join({db$designs %>%\n      filter(primary_purpose %in% c(\"Treatment\", \"Prevention\")) %>%\n      select(nct_id, intervention_model, primary_purpose)},\n      by = \"nct_id\") %>%\n  mutate(completion_days = \n           as.numeric(completion_date - start_date),\n         completion_months = completion_days / 30.417) %>% \n  filter(completion_days > 0)\n\n\n\nThe filtered data is a fraction of the original database, but still\ncontains a large number of trials to further investigate.\n\n\nn_distinct(selected_trials$nct_id)\n\n\n[1] 31955\n\nGroup trials by therapeutic area\nI wanted to split the data by therapeutic area (e.g., cancer,\ninfectious diseases). The database has MeSH terms in the\nbrowse_conditions table that are populated by the National\nLibrary of Medicine using an algorithm (according to the data\ndictionary). The conditions table has the data as submitted\nto clinicaltrials.gov (not drawn from a controlled vocabulary).\nMeSH\nis a controlled vocabulary for medical terms created by the US National\nLibrary of Medicine. One can use MeSH RDF to\nquery MeSH terms. Initially, I thought this would be an efficient way to\ncategorize trials by therapeutic area, but I did not end up using that\ndata.\nAfter looking through this data for some time, it seems that the MeSH\nterms in browse_conditions are sometimes overly liberal and add\nterms that don’t seem applicable. Therefore, I used the\nconditions table and came up with a list of terms to search for\ninstead of more systematic filtering by MeSH terms.\n\nIt would merit a more careful consideration of “Healthy” for phase 1\nstudies. “Healthy” is the most common condition in this dataset.\nOverall, I believe this approach is much too conservative and I am\nmissing trials for a given therapeutic area.\n\n\nterms_tas <- data.frame(\n  \"therapeutic_area\" = c(\"oncology\", \"cardiovascular\", \"metabolic\"),\n  \"pattern\" = c(\n    \"carcinoma|glioblastoma|glioma|tumor|cancer|leukemia|\n    leukaemia|lymphoma|melanoma|neoplasm|myeloma|malignancy|metastasis\",\n    \"cardio|artery|ventricular|ischemia|angina|\n    aortic|arteriosclerosis|aneurysm|myocardial|hypertension|\n    stroke|coronary|heart[^b]|cardiovascular|atrial\",\n    \"obesity|diabetes|metabolic|overweight|insulin\"\n  )\n)\n\n\n\nCreate some plots\nSome notes on the plot:\nI converted days to months by dividing by 30.417.\nI created a new data.frame with the median durations by phase to\nmake adding the text and arrow annotations easier.\nI tried to guess a good place for the text annotations based on the\ndata in a simple way. There are many ways to do this. I could have used\nx = Inf, y = Inf to place text in the\ntop-right corner, but this was not compatible with adding the\narrow.\nI made a quick function (convert_duration_text) to\nconvert the duration to easier-to-understand units. In other words,\nconvert 1039 days to “2 yrs 10 mos”. I am guessing there is an alternate\nmethod to do this with existing packages like {lubridate}.\n\n\nShow code\n\n# make helper functions to calc and plot\nconvert_duration_text <- function(days) {\n  results <- rep(NA_character_, length(days))\n  \n  results[days < 31] <- {\n    day_label <- ifelse(days[days < 31] == 1, \"day\", \"days\")\n    paste(days[days < 31], day_label)\n  }\n  \n  results[days >= 31 & days < 365] <- {\n    months <- round(days[days >= 31 & days < 365] / 30.417, 1)\n    month_label <- ifelse(months == 1, \"mo\", \"mos\")\n    paste(months, month_label)\n  }\n  \n  results[days >= 365] <- {\n    years <- floor(days[days >= 365] / 365.25)\n    left_days <- days[days >= 365] %% 365.25\n    months <- round(left_days / 30.417)\n    year_label <- ifelse(years == 1, \"yr\", \"yrs\")\n    month_label <- ifelse(months == 1, \"mo\", \"mos\")\n    paste(years, year_label, months, month_label)\n  }\n  \n  return(results)\n}\n\n# Plotting function\nplot_time_by_ta <- function(data, therapeutic_area) {\n  \n  ta_pattern <- terms_tas$pattern[terms_tas$therapeutic_area == therapeutic_area]\n  \n  filt_data <- data %>%\n    tidyr::unnest(conds) %>% \n    filter(grepl(ta_pattern, downcase_name)) %>% \n    distinct(nct_id, phase, completion_date, completion_days,\n             completion_months, start_date) %>% \n    filter(!is.na(completion_days)) %>% \n    group_by(phase) %>% \n    mutate(groups = paste0(phase, \"\\nn = \", n())) %>% \n    ungroup()\n  \n  filt_medians <- filt_data %>% \n    group_by(groups) %>% \n    summarise(medians = median(completion_months),\n              peak_at_med = sum(between(completion_months,\n                                        medians - 12, medians)),\n              median_days = median(completion_days)) %>% \n    # set the position for the labels\n    mutate(x1 = medians + 40, \n           x2 = medians, \n           y1 = max(peak_at_med) * 0.85, \n           y2 = max(peak_at_med) * 0.8, \n           median_label = paste0(\"median:\\n\",\n                                 convert_duration_text(median_days)))\n  \n  filt_data %>% \n    ggplot(aes(x = completion_months, fill = groups)) + \n    geom_histogram(position = \"identity\", alpha = 1, binwidth = 12) + \n    geom_vline(aes(xintercept = medians),\n               data = filt_medians,\n               linetype=\"dashed\", color = \"gray40\") +\n    geom_text(aes(label = median_label,\n                  x = x1,\n                  y = y1),\n              data = filt_medians, \n              size = 3,\n              nudge_x = 3, nudge_y = 3,\n              hjust = 0, vjust = 0,\n              family = \"Avenir Next\") +\n    geom_curve(aes(x = x1, y = y1, xend = x2, yend = y2),\n               data = filt_medians,\n               color = \"gray40\",\n               curvature = -0.3,\n               arrow = arrow(length = unit(0.05, \"npc\"), type = \"closed\")) + \n    scale_fill_manual(values = palette_colors) + \n    facet_wrap(~groups, nrow=1) +\n    labs(x = \"time in months\",\n         y = \"# of trials\", \n         title = \"How long do clinical trials take?\",\n         subtitle = paste(therapeutic_area, \"therapeutic area\")) + \n    theme(legend.position = \"none\",\n          panel.grid.minor = element_blank(),\n          text = element_text(size=11),\n          strip.text.x = element_text(size = 13))\n}\n\n\n\n\n\nplot_time_by_ta(selected_trials, \"oncology\")\n\n\n\n\n\n\nplot_time_by_ta(selected_trials, \"cardiovascular\")\n\n\n\n\nOverall, these numbers align with expectations from other studies\nlike Wong et al. They show in Supplemental Table 17 the median\nduration for different therapeutic areas. My understanding is that this\nanalysis included all trials (including non-industry sponsored).\n\nMy estimates are shorter, which I believe is due mainly to the filters I\napplied.\nSupplemental Table 17 from Wong et al.,\n2019Do biomarkers\nimprove the probability of success?\nClinicaltrials.gov data contains information on the inclusion and\nexclusion criteria for each study. However, it is currently challenging\nto extract information automatically on trial design, such as the type\nof biomarkers used.\nWong et al.,\n2019 presents data on the probability of success of clinical trials\nconditioned on biomarker use. The authors used citeline\ndata. I do not have access to this data, but the authors provided some\nsummary data in the paper that I can re-plot. I used the aggregated\nsummary data from Table 3.\n\nTable 3. POS of drug development programs with and without\nbiomarkers, using data from January 1, 2005, to October 31, 2015,\ncomputed using the phase-by-phase method. These results consider only\ntrials that use biomarkers in patient stratification. Since for the\nmajority of trials using biomarkers (92.3%) their status is observed\nonly on or after January 1, 2005, the choice of the time period is to\nensure a fair comparison between trials using and not using biomarkers.\nSE denotes standard error.\n\nHere, I am just plotting the oncology results.\nSome notes on the plot:\nI use stringr::str_wrap() to easily convert long facet\nlabels to less wide versions that won’t overlap each other\nI move the legend to a custom position and also move the facet\nlabels\nLoad and plot the data\n\n\nShow code\n\nwong_t3 <- read_csv(\"data/wongetal_all_table3.csv\")\n\nwong_t3 <- wong_t3 %>% \n  filter(category != \"All\",\n         TA == \"Oncology\") %>% \n  select(-total_phase_transitions) %>%\n  mutate(phase = stringr::str_wrap(phase, width = 10),\n         phase = forcats::fct_inorder(phase)) %>% \n  group_by(phase) %>% \n  mutate(percent_increase = paste0(\"+\", POS[2] - POS[1], \"%\")) %>% \n  ungroup()\n\nwong_t3 %>% \n  ggplot(aes(x = category, y = POS, fill = category)) +\n  geom_col()+\n  geom_errorbar(\n    aes(ymin = POS - SE, ymax = POS + SE), \n    width = 0.7,\n    color = \"gray40\") + \n  geom_curve(\n    aes(x = 1, y = `POS_No biomarker` + 7,\n        xend = 1.5, yend = `POS_With biomarker`),\n    data = tidyr::pivot_wider(\n      wong_t3, \n      names_from = category, \n      values_from = c(\"POS\", \"SE\")),\n    inherit.aes = F,\n    color = \"gray40\",\n    linetype = \"solid\",\n    size = 0.5,\n    curvature = -0.2,\n    arrow = arrow(length = unit(0.1, \"npc\"))) +\n  geom_text(\n    aes(label = percent_increase, y = POS + SE),\n    data = filter(wong_t3, category == \"No biomarker\"),\n    family= \"Avenir Next\", \n    size = 3.5,\n    nudge_y = 3 ) +\n  scale_fill_manual(values = palette_colors) +\n  facet_wrap(~phase, nrow = 1, strip.position = \"bottom\") + \n  labs(x = NULL,\n       y = \"POS (% +/- SE)\", \n       fill = NULL) +\n  theme(legend.position = c(0.15, 0.85), \n        legend.background = element_rect(fill=\"white\", color = \"white\"),\n        axis.text.x = element_blank(),\n        strip.text.x = element_text(size = 11))\n\n\n\n\nSome thoughts\nThis high-level view does not consider the time it takes to\noptimally position a therapy to reach the right patients that\ncould benefit. (It is a valuable exercise to plot the cumulative number\nof trials for a given investigational or approved drug over time and the\nnumber of new approvals for that drug.)\nThe clinicaltrials.gov data and the AACT database are tremendous\nresources. However, generally, you need to join in some orthogonal data\nto answer questions. For example, you cannot easily parse from the data\nmodel which interventions were the investigational arm versus the\ncomparator arms (if applicable). Some of the curation needed to augment\nthe data is available through proprietary databases and ontologies that\ncan better resolve drug name synonyms and classify trials and therapies\nin greater detail.\nsessionInfo\n\n\nsessionInfo()\n\n\nR version 4.0.5 (2021-03-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n[1] ggplot2_3.3.6 readr_1.4.0   dplyr_1.0.9  \n\nloaded via a namespace (and not attached):\n [1] highr_0.9         pillar_1.8.0      bslib_0.2.5.1    \n [4] compiler_4.0.5    jquerylib_0.1.4   forcats_0.5.1    \n [7] tools_4.0.5       digest_0.6.29     downlit_0.4.0    \n[10] gtable_0.3.0      jsonlite_1.8.0    evaluate_0.15    \n[13] memoise_2.0.0     lifecycle_1.0.1   tibble_3.1.8     \n[16] pkgconfig_2.0.3   rlang_1.0.4       cli_3.3.0        \n[19] DBI_1.1.1         rstudioapi_0.13   distill_1.3      \n[22] yaml_2.3.5        xfun_0.31         fastmap_1.1.0    \n[25] withr_2.5.0       stringr_1.4.0     knitr_1.39       \n[28] generics_0.1.3    vctrs_0.4.1       sass_0.4.0       \n[31] hms_1.0.0         grid_4.0.5        tidyselect_1.1.2 \n[34] glue_1.6.2        R6_2.5.1          fansi_1.0.3      \n[37] rmarkdown_2.11    farver_2.1.1      tidyr_1.2.0      \n[40] purrr_0.3.4       magrittr_2.0.3    scales_1.2.0     \n[43] htmltools_0.5.1.1 ellipsis_0.3.2    assertthat_0.2.1 \n[46] colorspace_2.0-3  labeling_0.4.2    utf8_1.2.2       \n[49] stringi_1.7.8     munsell_0.5.0     cachem_1.0.5     \n[52] crayon_1.5.1     \n\n\n\n\n",
    "preview": "posts/2022-10-02-replotting-clinical-trials/replotting-clinical-trials_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2022-10-02T09:37:03-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-09-04-what-pharma-companies-tweet-about-data-science/",
    "title": "Which pharma companies tweet about data science?",
    "description": "Mining some twitter data for trends and differences.",
    "author": [
      {
        "name": "Alice Walsh",
        "url": {}
      }
    ],
    "date": "2022-09-04",
    "categories": [],
    "contents": "\nI am interested in the rise of data science across all industries and\nthe growing demand for data-intensive work. In particular, I am\ninterested in pharmaceutical development (where I work) for a couple of\nreasons:\nData-intensive work is not “new” to Pharmaceutical development.\nBiostats teams are essential to designing clinicial trials, analyzing\nthese trials, and reporting the results. Likewise, research teams in\npreclinical R&D have also generated and processed large data sets\nfor many years.\nI get a lot of ads/tweets/etc. about the potential of AI/ML/Data for\ndrug discovery and development. I am in the optimistic camp when it\ncomes to how better technology and methods can unlock scientific\nbreakthroughs. But, I am also weary of unrealistic expectations and the\nabuse of “AI” to seem innovative or as a marketing tool.\nHere I looked at twitter data as a publicly-available data source to\nconfirm or reject my hypotheses around\nWhich companies are promoting AI/ML/Data as part of their\nmarketing\nWhether the use of data science terms has increased in the last few\nyears\nMethods\nI used the great {rtweet} package to\ncollect recent tweets from companies official twitter accounts. It had\nbeen several years since I used this package, but thankfully, my old\ncode worked fine. I would recommend Will\nChase’s post on this topic as an intro, which I found very helpful\nwhen I got started.\nMost companies have multiple twitter accounts. I tried to be fair and\nuse each company’s “main” account, but sometimes there was also a\n“science at xyz” account that seemed interesting. So I also looked at\nthose.\nI chose a list of top\nPharma companies based on sales. I manually found their twitter\nhandles.\nI also added a representative public biotech, Recursion, to serve as\na comparison for a company that I expected to have a lot of content\nabout data and machine learning.\n\n\nph_usernames <- c(\n  \"janssenglobal\",\n  \"pfizer\", \n  \"roche\",\n  \"abbvie\",\n  \"novartis\", \"novartisscience\",\n  \"merck\",\n  \"bmsnews\", \"scienceatbms\",\n  \"gsk\",\n  \"sanofi\", \"sanofiscience\",\n  \"astrazeneca\",\n  \"takedapharma\",\n  \"lillypad\",\n  \"recursionpharma\") \n\n\n\nI retrieved the last 3,200 tweets from each account.\n\n\nph_tweets <- get_timeline(ph_usernames, n=3200)\n\n\n\n\n\n\nThis is a lot of tweets, but some are retweets.\n\n\nnrow(ph_tweets)\n\n\n[1] 47977\n\nI combined the tweet data for companies where I queried both the\n“main” account and the “science” account.\n\n\nph_tweets <- ph_tweets %>% \n  mutate(screen_name = case_when(\n    screen_name %in% c(\"bmsnews\",\"ScienceAtBMS\") ~ \"bmsnews|ScienceAtBMS\",\n    screen_name %in% c(\"Novartis\",\"NovartisScience\") ~ \"Novartis|NovartisScience\",\n    screen_name %in% c(\"sanofi\", \"SanofiScience\") ~ \"Sanofi|SanofiScience\",\n    TRUE ~ screen_name\n  ))\n\n\n\nWho tweeted about data\nscience the most?\nI can calculate the number of tweets per company that contain certain\nwords.\n\n\ncount_tweets_by_word <- function(data, search_words) {\n  data %>% \n    # no retweets\n    filter(!is_retweet) %>%\n    # remove urls\n    mutate(text_filt = \n             stringr::str_replace_all(text,\"https?://t.co/[A-Za-z\\\\d]+|&amp;\", \"\")) %>% \n    mutate(tweet_pos = stringr::str_detect(text_filt, \n                                           paste(search_words, collapse = \"|\"))) %>% \n    group_by(screen_name) %>% \n    summarise(n_pos = sum(tweet_pos),\n              n_neg = sum(!tweet_pos), \n              .groups = \"drop\") %>%\n    mutate(perc_pos = scales::percent(n_pos / (n_pos + n_neg))) %>% \n    arrange(desc(n_pos / (n_pos + n_neg)))\n}\n\n\n\nLet’s test with “data”.\n\n\ncount_tweets_by_word(ph_tweets, search_words = \"[Dd]ata\") %>% \n  DT::datatable(rownames = FALSE,\n                colnames = c(\"Twitter Handle\", \"Tweets w/ words\",\n                             \"Tweets w/o words\", \"Percent w/ words\"))\n\n\n\n\nWe can get more specific and count tweets with the key words “data\nscience”, “AI”, and “machine learning”. I went ahead and included\n“statistics,” but sadly, there were very few tweets containing that\nword.\n\n\nsearch_words <- c(\"[Dd]ata [Ss]cience\",\n                  \"[Mm]achine [Ll]earning\",\n                  \"[ #]AI[^(DAB)]\",\n                  \"[Ss]tatistics\")\n\ncount_tweets_by_word(ph_tweets, search_words = search_words) %>% \n  DT::datatable(rownames = FALSE,\n                colnames = c(\"Twitter Handle\", \"Tweets w/ words\",\n                             \"Tweets w/o words\", \"Percent w/ words\"))\n\n\n\n\nLet’s check out some example tweets. Pfizer only had two data science\ntweets in our search:\n\n\nWe’re leading the charge in\n#AI\nwith the development of EstimATTR. This tool aims to educate healthcare\nproviders about combinations of cardiac and non-cardiac conditions known\nto be associated with wild-type ATTR-CM.\n\n— Pfizer Inc. (@pfizer)\nJanuary\n9, 2021\n\nAstraZeneca had 46 tweets. Let’s check out their most recent tweet\nabout machine learning:\n\n\nMachine learning allows us to identify new targets for novel medicines.\nOur scientists are applying these methods and accelerating cancer drug\ndiscovery by combining CRISPR and AI.\n#ICML2022\nhttps://t.co/KXemq7ytc4\npic.twitter.com/IwdxSoLa6h\n\n— AstraZeneca (@AstraZeneca)\nJuly\n17, 2022\n\nOverall, some companies are tweeting more about data science\nthan other companies! However, the total number and the\nfraction of total tweets that are about data science are tiny.\nFor AstraZeneca (the top tweeter), what were the top words they used\noverall?\n\n\nplot_top_words(\"astrazeneca\")\n\n\n\n\nWhen did\neveryone start tweeting about data science?\nI wanted to look at trends over time, but I was guessing that this\nwould not be feasible given the limitation of the twitter API returning\nthe most recent 3,200 tweets.\nHowever, the oldest tweets returned are from 2014, so I decided to do\na quick look. This is a bit lazy as the earliest tweet available from\neach company varies because they tweet at different frequencies:\nRecursion and Novartis less frequently and Janssen more frequently.\nConsidering the big pharma companies, the first appearance of the\nterms “data science”, “machine learning”, “AI”, or “statistics” was in\n2017.\nThe trend for these terms was increasing until 2020. I can speculate\nthat there may have been a lot of COVID-related tweets in 2020 and 2021,\nbut the data science tweets are back more than ever before in 2022!\n\n\nShow code\n\nph_tweets %>% \n  # filter for our interesting tweets\n  filter(screen_name != \"RecursionPharma\") %>%\n  mutate(text_filt = \n           stringr::str_replace_all(text, \"https?://t.co/[A-Za-z\\\\d]+|&amp;\", \"\")) %>% \n  mutate(text_pos = stringr::str_detect(text_filt, \n                                        paste(search_words, collapse = \"|\"))) %>%\n  # aggregate by year\n  group_by(text_pos,\n           year = lubridate::floor_date(created_at, unit = \"year\")) %>% \n  summarise(tweets = n(),\n            .groups = \"drop\") %>%\n  tidyr::complete(text_pos, year, \n                  fill = list(tweets = 0)) %>%\n  group_by(year) %>% \n  summarise(tweets = tweets[text_pos] / sum(tweets),\n            .groups = \"drop\") %>% \n  mutate(year = as.numeric(substr(as.character(year), 1, 4))) %>%\n  filter(year > 2016) %>% \n  ggplot(aes(x = factor(year), y = tweets)) + \n  geom_col() +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(title = \"Prevalence of tweets about data science\",\n       subtitle = \"Recent tweets from 12 pharma companies\",\n       y = \"Percent of all tweets\", x = NULL)\n\n\n\n\n\nI also looked at the number of likes and retweets, but I didn’t identify\nany interesting patterns.\nFinally, I can break the tweets down by company. These are the\nresults for the companies with at least ten tweets with my key\nwords.\nWhile AstraZeneca was sending a lot of data science flavored tweets\nin 2019, there aren’t many tweets on the subject for the last 3 years.\nMeanwhile, Janssen has picked up the torch and is on track to tweet on\ndata science topics more than anyone this year.\nThese numbers are very small and so you shouldn’t draw any serious\nconclusions. I am guessing these changes could be explained by a change\nof staff on the communications team rather than a major company\nstrategy!\n\n\nShow code\n\ntop_cos <- count_tweets_by_word(ph_tweets, \n                                search_words = search_words) %>% \n  filter(n_pos > 10) %>% pull(screen_name)\n\ntweet_by_co <- ph_tweets %>% \n  # filter for our interesting tweets\n  filter(screen_name %in% top_cos,\n         screen_name != \"RecursionPharma\") %>%\n  mutate(text_filt = \n           stringr::str_replace_all(text, \"https?://t.co/[A-Za-z\\\\d]+|&amp;\", \"\")) %>% \n  filter(stringr::str_detect(text_filt, \n                             paste(search_words, collapse = \"|\"))) %>%\n  # aggregate by year & company\n  group_by(\n           year = lubridate::floor_date(created_at, unit = \"year\"),\n           screen_name) %>% \n  summarise(tweets = n(),\n            .groups = \"drop\") %>%\n  mutate(year = as.numeric(substr(as.character(year), 1, 4))) \n\ntweet_by_co %>% \n  ggplot(aes(x = year, y = tweets, color = screen_name)) + \n  geom_point(show.legend = F) +\n  geom_line(aes(group = screen_name), show.legend = F) + \n  geom_text(aes(label = screen_name, y = tweets, color = screen_name), \n            inherit.aes = FALSE,\n            data = filter(tweet_by_co, year == \"2022\"),\n            x = 2022.1, \n            hjust = 0, check_overlap = TRUE,\n            position = position_nudge(y = 0.5),\n            family = \"Avenir\",\n            show.legend = F) + \n  scale_x_continuous(expand = expansion(add = c(0.1, 3)),\n                     breaks = 2015:2022) + \n  scale_y_continuous(breaks = seq(0, 30, 5)) +\n  labs(title = \"Number of tweets about data science\",\n       y = \"Tweets\", x = NULL) +\n  theme(panel.grid.minor.x = element_blank(),\n        panel.grid.major.x = element_blank())\n\n\n\n\nWhat’s next?\nThis project was a fun thing to look at over a long weekend. It\nconfirmed my suspicions that some companies overall tweet more than\nothers, and that some companies are promoting their data science more\nthan others.\nI hope to post more on this topic by looking at job descriptions,\nLinkedIn, and clinical trial data to examine more deeply the impact of\ndata-intensive work on pharma R&D.\nThis post was updated 2022-10-07 to improve a couple of plots\nthat I didn’t like and improve the regex.\nsessionInfo\n\n\npander::pander(sessionInfo())\n\n\nR version 4.0.5 (2021-03-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale:\nen_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats,\ngraphics, grDevices, utils,\ndatasets, methods and base\nother attached packages: tidytext(v.0.3.1),\nggplot2(v.3.3.6), dplyr(v.1.0.9) and\nrtweet(v.0.7.0)\nloaded via a namespace (and not attached):\ntidyselect(v.1.1.2), xfun(v.0.31),\nbslib(v.0.2.5.1), pander(v.0.6.3),\npurrr(v.0.3.4), lattice(v.0.20-41),\ncolorspace(v.2.0-3), vctrs(v.0.4.1),\ngenerics(v.0.1.3), htmltools(v.0.5.3),\nSnowballC(v.0.7.0), yaml(v.2.3.5),\nutf8(v.1.2.2), rlang(v.1.0.4),\njquerylib(v.0.1.4), pillar(v.1.8.0),\nglue(v.1.6.2), withr(v.2.5.0), DBI(v.1.1.1),\nlifecycle(v.1.0.1), stringr(v.1.4.0),\nmunsell(v.0.5.0), gtable(v.0.3.0),\nhtmlwidgets(v.1.5.3), memoise(v.2.0.0),\nevaluate(v.0.15), labeling(v.0.4.2),\nknitr(v.1.39), fastmap(v.1.1.0),\ncrosstalk(v.1.1.1), curl(v.4.3.2),\nfansi(v.1.0.3), highr(v.0.9),\ntokenizers(v.0.2.1), Rcpp(v.1.0.9),\nopenssl(v.2.0.2), scales(v.1.2.0),\nDT(v.0.18), cachem(v.1.0.5),\njsonlite(v.1.8.0), farver(v.2.1.1),\ndistill(v.1.3), askpass(v.1.1),\ndigest(v.0.6.29), stringi(v.1.7.8),\ngrid(v.4.0.5), cli(v.3.3.0), tools(v.4.0.5),\nmagrittr(v.2.0.3), sass(v.0.4.1),\ntibble(v.3.1.8), janeaustenr(v.0.1.5),\ncrayon(v.1.5.1), tidyr(v.1.2.0),\npkgconfig(v.2.0.3), downlit(v.0.4.0),\nellipsis(v.0.3.2), Matrix(v.1.3-2),\nlubridate(v.1.8.0), assertthat(v.0.2.1),\nrmarkdown(v.2.11), httr(v.1.4.3),\nrstudioapi(v.0.13), R6(v.2.5.1) and\ncompiler(v.4.0.5)\n\n\n\n\n",
    "preview": "posts/2022-09-04-what-pharma-companies-tweet-about-data-science/what-pharma-companies-tweet-about-data-science_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2022-10-07T08:41:46-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-07-26-rstudio-conference-talk-2022/",
    "title": "RStudio Conference Talk 2022",
    "description": "Becoming creative: How I designed a quilt with R.",
    "author": [
      {
        "name": "Alice Walsh",
        "url": {}
      }
    ],
    "date": "2022-07-26",
    "categories": [],
    "contents": "\nUpdate: The talk recording is now available online on the conference\nwebsite.\nI am excited to attend my first in person R-centric conference this\nyear with RStudio::conf.\nI am even more excited to get the opportunity to participate as a\nspeaker.\nLink to slides\nAbstract\n\nWhen someone asks about essential skills for data careers, I often\nhear responses like R, Python, and machine learning. However, I argue\nthat creativity is an underrated skill that you can and should practice.\nIn this talk, I want to tell you a story about a project I did to\nstretch my creative brain and use my favorite tool, R. I designed a\nquilt in R using generative art ideas. Then I created individual blocks\nthat make up the larger design. I used foundation paper piecing, a\nmethod that allows for intricate designs but has geometrical\nconstraints. I hope my talk will entertain and inspire folks to exercise\ntheir creative muscles to improve their performance and enjoyment of\ntheir day jobs.\n\nThanks to the organizers for having me and making this a very\npositive experience!\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-18T13:47:26-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-18-data-science-hangout/",
    "title": "Hanging out with data people",
    "description": "I co-hosted a \"Data Science Hangout\" in May.",
    "author": [
      {
        "name": "Alice Walsh",
        "url": {}
      }
    ],
    "date": "2022-06-18",
    "categories": [],
    "contents": "\nI was the “co-host” for a Data Science\nHangout a few weeks ago in May. These are weekly informal\nconversations for data science leaders hosted by Rachael Dempsey and\nRStudio. Thanks Rachael for inviting me! You can find the recording of\nthe event over on RStudio’s YouTube.\n\n\nRachael and RStudio did a fantastic job recapping the conversation in\nthe video description. However, I will add a rough outline of the topics\ncovered here.\n1. Optimizing remote work\nOne difference between remote work and in-office work is the\nreduction of thinking time away from screens. The book “Rest: Why You\nGet More Done When You Work Less” by Alex Soojung-Kim Pang was\nrecommended, and I plan to read it (ironically when I have more time and\nless work).\nI shared some ideas for effective remote brainstorming (a topic I am\ninterested in). One of the great benefits of remote teams is the ability\nto leverage technology to “level the playing field” for people of\ndifferent personality types, communication styles, and work\nlocations.\n2. Getting\nbetter data versus imputing missing data\nThere was a lot of good chat about imputing missing data and\nrecommendations for R packages. Unfortunately, the zoom chat is not\ncaptured in the YouTube replay - one of the benefits of joining\nlive!\nA good reminder here about not always trying to “fix” your analysis\nbut seeing if you can “fix” your data. Sometimes that missing data could\npoint to a problem in your data generation that is worth\ninvestigating.\n3. Conferences, tech,\nand leadership styles\nSomeone asked about good conferences to attend. There were some\nexcellent recommendations and another data point that many people are\nexcited about the possibility of attending in-person meetings\nagain.\nTech stacks and teams switching from one set of tools (SAS) to\nanother (R). There were some great tips from others who have been\nthrough this transition.\nResources to get started in data science. The recommendations\nincluded finding something you are excited about and choosing something\nto focus on.\nLeadership styles and communicating with stakeholders. My advice was\nto be opinionated and make it clear what you want your audience to take\naway from any analysis/presentation/report.\n4. Hiring (emphasis on remote\nhiring)\nMy aspiration is to make our interview experience better for\ncandidates. I have been doing remote interviews for a while now, but I\nstill struggle. I hope to continue to innovate and draw from the\ncollective wisdom about what works and what doesn’t.\nThe YouTube video has an excellent summary of this part. It seems\nthe consensus is split on whether a technical component (live interview,\ntake-home assignment, etc.) is valuable or not.\nOne idea I like is to have the candidate bring their own example\ncode/analysis and walk through it as part of the interview.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-06-18T13:05:44-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-05-how-does-one-come-up-with-new-ideas/",
    "title": "How does one come up with new ideas?",
    "description": "Useful tips I learned from the book \"Zig Zag\" by Keith Sawyer.",
    "author": [
      {
        "name": "Alice Walsh",
        "url": {}
      }
    ],
    "date": "2022-06-05",
    "categories": [],
    "contents": "\nA book that stuck in my head is “Zig Zag: The Surprising Path to Greater\nCreativity” by Keith Sawyer. The book is both a quick read and a\ndense resource that you could return to in the future when you are in a\ncreative rut or need to solve a problem.\nComing up with new ideas when you have a challenge can be profoundly\ndaunting. So I am very excited to find new ways to come up with ideas. I\nhave used the two techniques described here and found them worth\nsharing.\nMethod one: transform other\nideas\nThe first is the SCAMPER method. Scamper\nstands for Substitute, Combine,\nAdapt, Magnify or Modify,\nPut to other uses, Eliminate,\nRearrange or Reverse.\n\nA short Wikipedia journey has informed me that SCAMPER may have been\ncreated by Alex Faickney Osborn in the 1950s. He was an advertising\nexecutive and he is credited with inventing brainstorming.\nThis is a process to get new ideas by taking existing ones and\ntransforming them. You go through each letter in SCAMPER and identify\nideas that use that verb.\nHere is an example that I created from my own life. I recently\nstarted quilting, and I needed new ideas for quilt designs (rather than\nusing an existing pattern). Note that this example works well because I\nam creating a physical thing. This method is less effective for abstract\nproblems.\nPrompt: I need a new quilt design\nSubstitute: I could use different colors or fabrics in an\nexisting design\n\nIdea is not that exciting or new\nCombine: Merge a couple of different patterns. Or combine\ntwo things I know about, like painting and quilting or programming and\nquilting\n\nThis idea was fascinating to me and I have created a few different\ndesigns now using R\nAdapt: I could convert a cross stitch pattern to a quilt\npattern or try to convert a painting or photo into a pattern\n\nA good one! I have been drawing inspiration from paintings\nMagnify or modify: Make a small quilt block into a\nhuge block. Modify a classic block by rotating or transforming into a\ndifferent shape\n\nThis idea feels like a good seed that could be built upon and improved\nEliminate: Quilt on a panel or solid fabric\nwithout piecing\n\nAlso a solid idea, but too basic\nReverse: Decompose an existing pattern and rearrange\nit\n\nThis also fell into the less exciting bucket once I reviewed all the\nideas\nLooking at the above ideas, I love a lot of them! The next step was\nto prioritize these ideas and then improve them further.\nMethod two: an idea quota\nThe second method I have used is very simple. You choose a problem\nand come up with three (or more if you are ambitious!) new\nideas/solutions every day for a week. I love this method because it can\nalso be collaborative. You could give everyone on a team a prompt, and\nthen everyone can share all their ideas at the end of the week.\nAnother wonderful thing about this method is that it eliminates the\ntendency I have to use my first idea. This process helps me consider\nmore ideas and take the time to dive into the problem. It is also highly\nchallenging to add more ideas after coming up with ten ideas.\n\n\n\n",
    "preview": "posts/2022-06-05-how-does-one-come-up-with-new-ideas/images/noun-increase-creativity-4287123.png",
    "last_modified": "2022-06-18T12:54:36-04:00",
    "input_file": {},
    "preview_width": 1200,
    "preview_height": 1200
  },
  {
    "path": "posts/2022-05-24-management-resources/",
    "title": "Managing data teams: resources",
    "description": "A collection of links and book recommendations.",
    "author": [
      {
        "name": "Alice Walsh",
        "url": {}
      }
    ],
    "date": "2022-05-24",
    "categories": [],
    "contents": "\nI wanted to gather some thoughts on managing computational research\nteams in biopharma (sometimes encompassing bioinformatics, computational\nbiology, and data science). Mostly, I want to keep track of various\nlinks and books and grow the list over time.\nRecommendations\nResilient Management\nby Lara Hogan\nThis book is an excellent overview of how to be a manager written\nwith a first-time manager in mind. However, it contains lots of good\nreminders for those with management experience.\n\nThe\nManager’s Path by Camille Fournier\nAnother great book - written for the tech audience.\n\nJacqueline Nolis has a great blog\nThere are a set of posts on hiring from 2017, which are very\nthoughtful.\nDr. Nolis also co-authored Build\na Career in Data Science.\n\nThe BICEPS model\nof core needs\nThis is a valuable framework to apply whenever you or someone you\nwork with reacts strongly to a situation.\nYou may have also heard of SCARF, which is more or less the same\nthing.\n\nGrowth Mindset\n“Growth mindset” was a common theme in various trainings I attended\nin the past. Some people react negatively to this concept, but they also\nhave a closed mindset.\n\nLearn about and practice active listening\nMaybe it sounds cliché, but listening is a skill that we could all\npractice more. I can struggle as an extrovert to be quiet and\nlisten.\nI would love to find some good resources on this (books, videos,\netc.). I have read a few ebooks to try to find a recommendation, but\nthis might be a topic that is better addressed with training or\nvideos.\n\nRead about the “bring\nme a rock” phenomenon\nI am not sure of the exact origins of the term “bring me a\nrock”\n\nThe\nLeadership Pipeline by Ram Charan\nThis book has been around for a while. I found some of the advice\nless relevant to research organizations, but the general concepts can be\nhelpful.\n\nBeing Glue by Tanya Reilly\nThis (very influential) tech talk has an excellent summary of many\npeople’s experiences at work where they are not rewarded for valuable\nwork. It is written from the perspective of a software engineer, but it\napplies more broadly to “technical” work.\n“Every senior person in an organization should be aware of the less\nglamorous - and often less-promotable - work that needs to happen to\nmake a team successful.”\n\nI am looking\nto add more resources/books/podcasts on\nCareer development resources - I am looking for some good materials\nthat are freely available for helping others with career planning.\nHiring and recruiting - what works well\nOther thoughts\nLately, I have been considering whether there is room for people to\nswitch between people leadership and technical leadership in\ncomputational research. Can you change from people management to an\nindividual contributor and then back again? My experience has been that\nthis is a less common path in Big Pharma/biotech (the industry where I\nam most familiar). However, this flexible path seems more common in tech\ncompanies. Maybe it can be the future for us as well? Personally, I went\nfrom a role where I was 90% managing teams and doing various other\n“leadership things” to joining a small company where I needed to provide\na lot of research contributions and strategy while we got off the\nground. Am I viewed as a good leader if I am too “in the weeds”? I also\nknow people who are perfectly capable of people management roles (maybe\neven great at them!), but the technical work energizes them and gets\nthem excited to show up every Monday morning.\n\n\n\n",
    "preview": "posts/2022-05-24-management-resources/images/noun-books-3647451.png",
    "last_modified": "2022-05-24T19:14:50-04:00",
    "input_file": {},
    "preview_width": 1200,
    "preview_height": 1200
  },
  {
    "path": "posts/2022-04-15-aacr-recap-2022/",
    "title": "Recap of AACR 2022",
    "description": "My notes on the AACR annual meeting, April 2022.",
    "author": [
      {
        "name": "Alice",
        "url": {}
      }
    ],
    "date": "2022-04-15",
    "categories": [],
    "contents": "\n\n\n\n\nA sketch of my experience in the convention center.\nI attended the American Association of Cancer Research (AACR) annual meeting in New Orleans from April 10th-13th. This was my first in-person conference in over two years!\nI am recapping my take-aways here at two levels:\nOn the ground: snippets of interesting science I saw\nFrom the clouds: thoughts on return to in-person meetings and networking\nOn the ground:\nCanine oncology\nThere was a presence from canine oncology companies, Fidocure and PetDx as well as posters from veterinary oncologists from the University of Florida (abstract 5030) and the University of Pennsylvania (abstract 1344 and 1363) among others.\nAccording to Fidocure, there are 6 million new cancer diagnoses in dogs annually versus less than 2 million cases in humans.\nI am interested in where this research develops. Canines are an exciting model for human cancer from a genomic similarity and environment similarity point of view. It is attractive to imagine the shared benefit of finding drugs that work both in humans and dogs.\n\nData science, AI, computational biology, etc.\nOverall, I didn’t see anything super exciting or anything super depressing. There was a good balance of approaches advanced by both academic and industry teams.\nThere was a session chaired by Trey Ideker called “Interpreting and Building Trust in Artificial Intelligence Models.” that included talks from Eli Van Allen and Su-In Lee. I enjoyed reading this preprint from Dr. Lee’s group after the meeting.\nThere was another session chaired by Olga Troyanskaya called “Artificial Intelligence in Cancer Research and Care” that included talks from Dana Pe’er and John Quackenbush.\nI also attended a mini-symposium called “Emerging Topics in Computational Oncology,” chaired by Ben Greenbaum, and Ben Raphael which featured several interesting talks on a broad range of topics.\n\nBiomarkers\nI enjoyed the talks from former BMS colleagues (Jonathan Baden and Jaclyn Neely) who discussed comparing bTMB and tTMB, and biomarker work from Nivolumab HCC clinical studies (abstracts 2134 and 2145).\nFoundation Medicine and collaborators from the University of Minnesota presented a poster on their latest HRD model (abstract 1249). Their model used XGBoost on DNA sequencing features to predict homologous recombination deficiency (HRD) and showed the predicted HRD across tumor types and the concordance with other HRD biomarkers (e.g., gLOH). This work is similar to models recently published in a preprint from Tempus that uses RNA features to predict HRD.\nSpatial transcriptomics and single-cell ’omics continue to capture the imagination of cancer researchers. There were a lot of posters and talks. I am excited to see that the challenges encountered by early adopters seem to be getting resolved. I am optimistic that additional (clinically actionable) insights will emerge beyond the technical “we can make the data.”\n\nFrom the clouds:\nI need new glasses. I haven’t looked at a screen farther than 20 inches away in a long time.\nI missed everyone! It was great connecting with present, past, and (hopefully) future colleagues and collaborators. Unfortunately, we haven’t figured out how to make virtual conferences as effective for creating and building connections.\nHighlighting new voices versus established researchers. Sometimes it can be boring to see the same stars giving similar talks, and I tend to skip those. I prefer talking with folks at their posters and chatting with other people after the sessions. I am thankful to all the people who were so friendly and welcoming to discuss ideas with me.\nDiversifying cancer research\nAACR gives awards to recognize and fund travel for minority researchers, and I hope that we can do more to diversify cancer research (both the researchers and the patients).\nThere were some fantastic posters and talks that highlighted the need for more diverse clinical trials and the differences in cancer patients based on genetic ancestry.\nCoincidentally, on April 13th, the FDA published new guidance on improving diversity in clinical trial participants.\n\nsessionInfo\n\n\npander::pander(sessionInfo())\n\n\nR version 4.0.5 (2021-03-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, utils, datasets, methods and base\nloaded via a namespace (and not attached): Rcpp(v.1.0.7), fansi(v.0.4.2), digest(v.0.6.29), R6(v.2.5.0), jsonlite(v.1.7.2), magrittr(v.2.0.1), evaluate(v.0.14), highr(v.0.9), stringi(v.1.5.3), rlang(v.0.4.10), cachem(v.1.0.5), jquerylib(v.0.1.4), bslib(v.0.2.5.1), vctrs(v.0.3.7), rmarkdown(v.2.11), distill(v.1.3), tools(v.4.0.5), pander(v.0.6.3), stringr(v.1.4.0), xfun(v.0.30), yaml(v.2.2.1), fastmap(v.1.1.0), compiler(v.4.0.5), memoise(v.2.0.0), htmltools(v.0.5.1.1), knitr(v.1.37), downlit(v.0.4.0) and sass(v.0.4.0)\n\n\n\n\n",
    "preview": "posts/2022-04-15-aacr-recap-2022/images/aacr_floor.png",
    "last_modified": "2022-04-16T09:44:53-04:00",
    "input_file": {},
    "preview_width": 640,
    "preview_height": 640
  },
  {
    "path": "posts/2022-03-30-quilting-part-2/",
    "title": "Quilting with R, part 2",
    "description": "More about using R to make a quilt.",
    "author": [
      {
        "name": "Alice",
        "url": {}
      }
    ],
    "date": "2022-04-06",
    "categories": [],
    "contents": "\n\nContents\nDesigning a quilt in R\nThe challenge - going\nfrom png to fabric\nConverting the ggplot\ninto a line drawing\nBreaking into pieces\nThe finished quilt top\nUpdates\nsessionInfo\n\n\n\nDesigning a quilt in R\nTo recap the previous\npost on this topic, I generated a quilt design in R using {ggplot2}\nand I was working through my overall plan:\nI wrote some functions to create a design using ggplot2 (post\n1)\nI wrote some code to turn my image into something that could be\nprinted as individual quilt blocks (this post)\nI made the actual quilt!\nI revisited how to make my process more general and implement\nsomething useful to design quilts that use foundation paper piecing\nIn this post, I will go through an overview of part 2.\nThe challenge - going\nfrom png to fabric\nOnce I had a design, I needed to figure out how to make the design\ninto something that could actually be pieced together from fabric.\nThe approach I used is called foundation\npaper piecing. This is a technique that enables a quilter to make\nvery intricate and precise designs by sewing fabric onto a piece of\npaper sequentially. This provides stability and precision by sewing\nalong a printed line.\nFoundation paper piecing (FPP for short) allows a quilter to make\nsome incredible creations! Here is a link to the\nwinners of this year’s Quiltcon awards. The people’s choice winner (a\nportrait of the artist’s family) was made using FPP.\nTo create an entire FPP quilt, you need to break your larger design\ninto smaller blocks that are “piece-able.” Here is the basic FPP\n“algorithm” (typically performed by hand):\nA hypothetical starting design (keep in mind that the final quilt\nwill actually be reversed as you sew onto the “wrong” side):\n\n\n\nBreak the larger design into blocks (usually into blocks that can be\nsewn together into row or columns)\n\n\n\nTest whether the blocks are “piece-able” - not all shapes can be\ncreated!\n\n\n\nIf the blocks are not “piece-able” - break into smaller\nblocks\nCreate the seam lines on the block and label the sections by\norder\n\n\n\nI am planning to save more information on steps 3 and 4 for a future\npost.\nHere, I will just focus on how to do step 2 easily in R.\nConverting the ggplot\ninto a line drawing\nMy previous function, save_my_image(), creates the\ndesign and can save an image.\n\n\ndesign <- save_my_image(out_path = NULL, # don't need to save it\n                   height_range = 10:40,\n                   width_range = 20:80,\n                   n_cubes = 3, \n                   n_second_color = 1,\n                   horizon_y = 80)\n\n\n\n\n\n\nI created a new function, create_pattern_pdf, that takes\nthe data.frame and plots as line drawing with labeled polygons. The next\nsection breaks down how that works.\nBreaking into pieces\nOne easy way to separate a large image into smaller blocks is just to\nuse a program that can do tiled printing (such as Adobe Reader).\nHowever, that doesn’t give as much flexibility as you might like to\nselect where to break up the image.\nBelow is how I turned a design - which was created on a 2D plane from\n(x,y) of (0,0) to (100,100) into a set of pdfs that were 8” by 8” to\nprint.\nRescale to the size of the final quilt\n\n\nlibrary(dplyr, quietly = TRUE) # used for data manipulations\nlibrary(ggplot2) # for plotting\nfinal_quilt_size <- 40 # units are arbitrary (I think in inches)\nscale_factor <- final_quilt_size / 100 # my image was \"100\" wide\nwidth_blocks <- final_quilt_size / 5   # I wanted 5 across, makes nice 8\"\n\n\n\nCreate a data.frame with the start and end x,y for each block\n\n\ngrid <- tidyr::crossing(x = seq(0, width_blocks * 4, width_blocks),\n                        y = seq(0, width_blocks * 4, width_blocks)) %>%\n  mutate(x_end = x + width_blocks,\n         y_end = y + width_blocks) %>%\n  arrange(x, y)\n\nhead(grid)\n\n\n# A tibble: 6 × 4\n      x     y x_end y_end\n  <dbl> <dbl> <dbl> <dbl>\n1     0     0     8     8\n2     0     8     8    16\n3     0    16     8    24\n4     0    24     8    32\n5     0    32     8    40\n6     8     0    16     8\n\nAdd in some information that was not in the design created\npreviously\n\n\n# Add in the horizontal line (I wanted it at y = 50)\nhorizontal <- data.frame(cube_id = c(\"\",\"\"),\n                         x = c(0, 100),\n                         y = c(50, 50)) %>%\n  mutate(x = x*scale_factor, y = y*scale_factor)\n# This ensures that the paths are \"closed\" without any open edges\nmissing_paths <- design %>% arrange(desc(cube_id))\n\n\n\nLoop through each block, plot and label, save out to a file. I\ntried to label each face an index to indicate color, but in my final\ndesign there were overlapping polygons, so colors needed to be manually\nchecked for each block.\n\n\nfor (i in seq_len(nrow(grid))) {\n  block <- design %>%\n    bind_rows(missing_paths) %>%\n    arrange(cube_id) %>%\n    mutate(x = x*scale_factor, y = y*scale_factor) %>%\n    group_by(id) %>%\n    mutate(ave_x = mean(x), ave_y = mean(y)) %>%\n    group_by(value) %>%\n    mutate(color = cur_group_id()) %>%\n    ungroup() %>%\n    ggplot() +\n    # horizon line\n    geom_path(aes(x = x, y = y, group = cube_id),\n              data = horizontal,\n              color = \"black\",\n              size = 1) +\n    geom_polygon(aes(x = x, y = y, group = id),\n                 fill = \"white\", color = \"blue\",\n                 size = 1,\n                 alpha = 1) +\n    geom_text(aes(label = color, x = ave_x, y = ave_y)) +\n    coord_equal(clip = \"off\",\n                xlim = c(grid$x[i], grid$x_end[i]),\n                ylim = c(grid$y[i], grid$y_end[i]),\n                expand = FALSE) +\n    theme_void() +\n    theme(legend.position = \"none\",\n          panel.border = element_rect(colour = \"black\", fill=NA, size=0.5))\n  ggsave(plot = block,\n         filename = paste0(\"to_print/\",i,\".pdf\"),\n         width = width_blocks, height = width_blocks)\n}\n\n\n\n One trick to plot only\ncertain limits of the image without removing data is to set the\nlimits within coord_equal().  Note that I\nused theme_void() again to remove most the axis information\nand gridlines.  Further, I added back in a panel.border,\nwhich will be very useful when I go to make the blocks.\nFor illustration purposes, here is what one of the blocks looked\nlike. Now I could print (at 100% scale) and the squares were a perfect 8\ninches.\n\n\n\nThe finished quilt top\nHere is a preview of the completed quilt top. I still need to\nactually quilt it, but the top does look like the original design!\n\n\n\nThe code is available in a GitHub repo.\nUpdates\nI wanted to add an image of the finished quilt and also point to the\npost on my RStudio::conf talk about quilting and creativity. I am\ninspired to try many more quilting ideas inspired by data or\nprogramming. Hopefully, I will add more posts with more details on this\nproject and other projects soon.\n\n\n\nsessionInfo\n\n\npander::pander(sessionInfo())\n\n\nR version 4.0.5 (2021-03-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale:\nen_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats,\ngraphics, grDevices, utils,\ndatasets, methods and base\nother attached packages: ggplot2(v.3.3.6)\nand dplyr(v.1.0.9)\nloaded via a namespace (and not attached):\nRcpp(v.1.0.9), bslib(v.0.2.5.1),\ncompiler(v.4.0.5), pillar(v.1.8.0),\njquerylib(v.0.1.4), highr(v.0.9),\ntools(v.4.0.5), digest(v.0.6.29),\ndownlit(v.0.4.0), gtable(v.0.3.0),\njsonlite(v.1.8.0), evaluate(v.0.15),\nmemoise(v.2.0.0), lifecycle(v.1.0.1),\ntibble(v.3.1.8), pkgconfig(v.2.0.3),\npng(v.0.1-7), rlang(v.1.0.4), cli(v.3.3.0),\nDBI(v.1.1.1), rstudioapi(v.0.13),\ndistill(v.1.3), yaml(v.2.3.5), xfun(v.0.31),\nfastmap(v.1.1.0), withr(v.2.5.0),\nstringr(v.1.4.0), knitr(v.1.39),\ngenerics(v.0.1.3), vctrs(v.0.4.1),\nsass(v.0.4.0), grid(v.4.0.5),\ntidyselect(v.1.1.2), glue(v.1.6.2),\nR6(v.2.5.1), fansi(v.1.0.3),\nrmarkdown(v.2.11), pander(v.0.6.3),\ntidyr(v.1.2.0), purrr(v.0.3.4),\nmagrittr(v.2.0.3), scales(v.1.2.0),\nhtmltools(v.0.5.1.1), assertthat(v.0.2.1),\ncolorspace(v.2.0-3), utf8(v.1.2.2),\nstringi(v.1.7.8), munsell(v.0.5.0) and\ncachem(v.1.0.5)\n\n\n\n\n",
    "preview": "posts/2022-03-30-quilting-part-2/images/finished_top.png",
    "last_modified": "2022-09-27T18:50:34-04:00",
    "input_file": {},
    "preview_width": 1267,
    "preview_height": 1280
  },
  {
    "path": "posts/2022-03-24-quilting-with-r/",
    "title": "Quilting with R, part 1",
    "description": "One of my hobbies is quilting, so I designed a quilt in R.",
    "author": [
      {
        "name": "Alice",
        "url": {}
      }
    ],
    "date": "2022-03-24",
    "categories": [],
    "contents": "\n\nContents\nDesigning a quilt in R - the original idea\nMaking some aRt\nThe basic principle\nGetting random\n\nFor future work\nsessionInfo\n\n\nDesigning a quilt in R - the original idea\nI took up quilting as a creative outlet near the beginning of 2021. After completing a few projects, I decided to make some original designs.\nAt the same time, I was seeing a lot of amazing #Rtistry projects shared on Twitter.\nSo, I thought I would try to create some generative art in R and transform it into a quilt!\nThis endeavor turned into a large project, roughly divided into the following steps:\nI wrote some functions to create a design using ggplot2\nI wrote some code to turn my image into something that could be printed as individual quilt blocks\nI made the actual quilt!\nI revisited how to make my process more general and implement something useful to design quilts that use foundation paper piecing\nIn this post, I will go through an overview of part 1.\nMaking some aRt\nI was inspired by the art of Fred Kaplan, who was my instructor for a couple of continuing education painting classes. Fred has created some fantastic imagined landscapes that feature geometric structures.\nI thought I could create some interesting polygons drawn with 2-point perspective in ggplot2. Then, I could use fabrics that suggest a light source with lighter values on some faces and darker values on others.\nHere is my final design:\n\n\n\nThe basic principle\nThe first thing I set to do was program the creation of some rectangular polygons drawn with 2-point perspective.\nThis can be achieved with {ggplot2} using geoms like geom_path(), geom_segment(),or geom_polygon().\nFirst, load some packages.\n\n\nlibrary(dplyr, quietly = TRUE) # for data manipulation, pipe\nlibrary(ggplot2)               # plot engine\nlibrary(retistruct)            # to get intersection of lines\n\nset.seed(45)\n\n\n\nA box is just 7 or 9 segments. To draw a box, you need to specify\nAn x location for the left, center, and right segments\nA y location for the top and bottom on the center segment\nThe location of the two vanishing points (Here I set the first to (0,0) and the second to (some value, 0))\nYou calculate all the segments’ start and stop x and y coordinates from this information. See below the function, make_new_cube, that performs this calculation to make a single box.\n\n\nShow code\n\nmake_new_cube <- function(xes, # a vector of three values between (0, vp)\n                          yes, # a vector of two values\n                          vp = 10) {\n  # pick the vanishing points (y == 0)\n  vp <- list(c(0, 0), c(vp, 0))\n  # make the center vertical segment\n  cube <- data.frame(x = xes[2],\n                     xend = xes[2],\n                     y = yes[1],\n                     yend = yes[2])\n\n  # add the left vertical segment\n  new_row <- c(xes[1],\n               xes[1],\n               (yes[1]/xes[2])*xes[1],\n               (yes[2]/xes[2])*xes[1])\n  cube <- rbind(cube, new_row)\n\n  # add the right vertical segment\n  new_row <- c(xes[3],\n               xes[3],\n               (yes[1]/(xes[2] - vp[[2]][1]))*(xes[3] -xes[2]) + yes[1],\n               (yes[2]/(xes[2] - vp[[2]][1]))*(xes[3] -xes[2]) + yes[2])\n  cube <- rbind(cube, new_row)\n\n  # add the top lines\n  new_row <- c(cube[1,1], cube[2,1], cube[1,3], cube[2,3])\n  cube <- rbind(cube, new_row)\n  new_row <- c(cube[1,1], cube[3,1], cube[1,3], cube[3,3])\n  cube <- rbind(cube, new_row)\n\n  # add the bottom lines\n  new_row <- c(cube[1,1], cube[2,1], cube[1,4], cube[2,4])\n  cube <- rbind(cube, new_row)\n  new_row <- c(cube[1,1], cube[3,1], cube[1,4], cube[3,4])\n  cube <- rbind(cube, new_row)\n\n  # if all above or below y = 0,  then get bottom or top segments\n  add_top <- all(c(cube$y, cube$yend) < 0)\n  add_bottom <- all(c(cube$y, cube$yend) > 0)\n\n  if (add_top) {\n    # intersect left bottom [2,3] to right vp and right bottom [3,3] to left vp\n    left_top <- c(xes[1], max(cube[2,3], cube[2,4]))  #left top\n    right_p <- vp[[2]] # right vp\n    right_top <- c(xes[3], max(cube[3,3], cube[3,4]))  #right top\n    left_p <- vp[[1]] # left vp\n    poss_top <- line.line.intersection(left_top, right_p,\n                                       right_top, left_p,\n                                       interior.only = TRUE)\n    new_row <- c(poss_top[1], left_top[1], poss_top[2], left_top[2])\n    cube <- rbind(cube, new_row)\n    new_row <- c(poss_top[1], right_top[1], poss_top[2], right_top[2])\n    cube <- rbind(cube, new_row)\n  }\n  if (add_bottom) {\n    # intersect left bottom [2,3] to right vp and right bottom [3,3] to left vp\n    left_b <- c(xes[1], min(cube[2,3], cube[2,4]))  #left bottom\n    right_p <- vp[[2]] # right vp\n    right_b <- c(xes[3], min(cube[3,3], cube[3,4]))  #right bottom\n    left_p <- vp[[1]] # left vp\n    poss_bottom <- line.line.intersection(left_b, right_p,\n                                          right_b, left_p, interior.only = TRUE)\n    new_row <- c(poss_bottom[1], left_b[1], poss_bottom[2], left_b[2])\n    cube <- rbind(cube, new_row)\n    new_row <- c(poss_bottom[1], right_b[1], poss_bottom[2], right_b[2])\n    cube <- rbind(cube, new_row)\n  }\n  return(cube)\n}\n\n\n\nLet’s test this function. It makes a nice data frame with our segments.\n\n\nnew_xes <- c(1, 2, 3) #left, center, right\nnew_yes <- c(4, 6) # bottom, top of center\n\nnew_cube <- make_new_cube(new_xes, new_yes, vp = 10)\nnew_cube\n\n\n    x xend        y yend\n1 2.0    2 4.000000 6.00\n2 1.0    1 2.000000 3.00\n3 3.0    3 3.500000 5.25\n4 2.0    1 4.000000 2.00\n5 2.0    3 4.000000 3.50\n6 2.0    1 6.000000 3.00\n7 2.0    3 6.000000 5.25\n8 1.6    1 1.866667 2.00\n9 1.6    3 1.866667 3.50\n\nFor the purpose of illustration, I will label the points on the plot.\n\n\nnew_cube %>%\n  ggplot() +\n  geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_label(aes(x = x, y = y, label = paste0(\"(\",x,\",\",y,\")\"))) +\n  theme_void()\n\n\n\n\nGetting random\nTo draw polygons in R where the faces would be colored, I rewrote the above make_new_cube function to be make_new_poly. To make it easier to conceptualize, this function default to a view that goes from (0,0) to (100,100). I also used colors that would suggest a light source, with darker colors on one side and lighter colors on the other.\nI then wrote a function save_my_image that\nTakes as arguments\nthe number of boxes to make\nthe number of these you want to be in an alternate color\nthe height and width ranges of the boxes\nYou can also customize the location of the horizon line and the colors as desired\n\nFor each box, it picks a random value for inputs of make_new_poly within the input height and width ranges\nCreates the “sky” and “ground” and adds all the boxes\nDisplays the image and (optionally) saves a copy to a file\nReturns the data frame with all the polygons\nHere is an example:\n\n\ndesign <- save_my_image(out_path = NULL, # don't need to save it\n                   height_range = 10:40,\n                   width_range = 20:80,\n                   n_cubes = 3, \n                   n_second_color = 1,\n                   horizon_y = 80)\n\n\n\n\n\n\nAnd another one:\n\n\ndesign <- save_my_image(out_path = NULL, # don't need to save it\n                   height_range = 60:80,\n                   width_range = 20:30,\n                   n_cubes = 6, \n                   n_second_color = 2,\n                   horizon_y = 40)\n\n\n\n\n\n\nFor future work\nI would have liked to have implemented cast shadows, but I think it would require a major overhaul of my framework. Briefly, you could calculate all the points in 3D and then convert them to a 2D projection.\nI could better structure and document these functions to make them easier to extend.\nThe code is available on GitHub here.\nsessionInfo\n\n\npander::pander(sessionInfo())\n\n\nR version 4.0.5 (2021-03-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, utils, datasets, methods and base\nother attached packages: retistruct(v.0.6.3), ggplot2(v.3.3.5) and dplyr(v.1.0.5)\nloaded via a namespace (and not attached): Rcpp(v.1.0.7), bslib(v.0.2.5.1), compiler(v.4.0.5), pillar(v.1.6.0), jquerylib(v.0.1.4), highr(v.0.9), magic(v.1.6-0), tools(v.4.0.5), digest(v.0.6.29), downlit(v.0.4.0), gtable(v.0.3.0), jsonlite(v.1.7.2), evaluate(v.0.14), memoise(v.2.0.0), lifecycle(v.1.0.0), tibble(v.3.1.0), pkgconfig(v.2.0.3), png(v.0.1-7), rlang(v.0.4.10), DBI(v.1.1.1), distill(v.1.3), yaml(v.2.2.1), xfun(v.0.30), fastmap(v.1.1.0), withr(v.2.4.2), stringr(v.1.4.0), knitr(v.1.37), ttutils(v.1.0-1), htmlwidgets(v.1.5.3), generics(v.0.1.0), vctrs(v.0.3.7), sass(v.0.4.0), grid(v.4.0.5), tidyselect(v.1.1.0), glue(v.1.4.2), R6(v.2.5.0), fansi(v.0.4.2), rgl(v.0.108.3), rmarkdown(v.2.11), pander(v.0.6.3), farver(v.2.1.0), purrr(v.0.3.4), magrittr(v.2.0.1), scales(v.1.1.1), htmltools(v.0.5.1.1), ellipsis(v.0.3.1), abind(v.1.4-5), assertthat(v.0.2.1), colorspace(v.2.0-0), labeling(v.0.4.2), utf8(v.1.2.1), geometry(v.0.4.5), stringi(v.1.5.3), munsell(v.0.5.0), cachem(v.1.0.5) and crayon(v.1.4.1)\n\n\n\n\n",
    "preview": "posts/2022-03-24-quilting-with-r/images/cubes_8_red_blue.png",
    "last_modified": "2022-03-25T09:49:38-04:00",
    "input_file": {},
    "preview_width": 4000,
    "preview_height": 4000
  },
  {
    "path": "posts/2022-03-20-more-models/",
    "title": "Testing many models with grouped data",
    "description": "Another example of something I do a lot and forget how to do.",
    "author": [
      {
        "name": "Alice",
        "url": {}
      }
    ],
    "date": "2022-03-20",
    "categories": [],
    "contents": "\n\nContents\nMany models!\nAn example: penguin linear models\nAnother example: survival models\nsessionInfo\n\n\n\nMany models!\nI often have a situation where I am testing many hypotheses.\nHow I tested many models in R in the past was to use lapply or a loop. I don’t think there is any problem with that approach, I just really like using a pattern these days with grouped data using tidyverse packages.\nThe general pattern is\nMake the data long (if not already long)\nGroup and nest\nMutate to calculate your statistics\nUn-nest, filter, or select to get your desired output\n There is a great vignette on this topic from the {broom} package.\n\nAnother day, I will add a post on operations on pairwise combinations of variables ala the {corrr} package.\nFirst, load some packages.\n\n\nlibrary(tidyr, quietly = TRUE) # manipulating data and nesting\nlibrary(dplyr, quietly = TRUE) # general data and piping\nlibrary(purrr, quietly = TRUE) # i will use purrr::map\nlibrary(broom)                 # very good at reformatting model objects\nlibrary(palmerpenguins)        # for more fun data\nlibrary(survival)              # for time to event models\nlibrary(ggplot2)               # to make our plots\ntheme_set(theme_minimal(base_family = \"Avenir\")) # for plot appearance\n\n\n\nAn example: penguin linear models\nLet’s say we are interested in the association between all the numeric variables in the {palmerpenguins} penguin dataset and the species.\nIf you are not familiar with this dataset, the three penguin species have different features like bill depth, bill length, body mass, and flipper length.\n\n\nhead(penguins, 4)\n\n\n# A tibble: 4 x 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm\n  <fct>   <fct>              <dbl>         <dbl>             <int>\n1 Adelie  Torgersen           39.1          18.7               181\n2 Adelie  Torgersen           39.5          17.4               186\n3 Adelie  Torgersen           40.3          18                 195\n4 Adelie  Torgersen           NA            NA                  NA\n# … with 3 more variables: body_mass_g <int>, sex <fct>, year <int>\n\nHere, we can see that Gentoo are some big penguins and that Adelie penguins have shorter bill length.\n\n\nShow code\n\npenguins %>% \n  pivot_longer(cols = where(is.numeric)) %>% \n  mutate(name = stringr::str_replace_all(name, \"_\", \" \"),\n         name = stringr::str_wrap(name, width = 10)) %>%\n  ggplot(aes(x = species, y = value, fill = species)) + \n  geom_boxplot() + \n  scale_fill_manual(values = c(\"#0E89BA\",\"#85BAA1\",\"#C16E70\")) +\n  facet_wrap(~name, scales = \"free\", nrow = 1) + \n  labs(title = \"The penguin species are different\",\n       x = NULL) +\n  theme(\n    legend.position = \"none\",\n    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)\n  )\n\n\n\n\nAs a reminder, I will follow the same general pattern above\nMake the data long - pivot all the numeric variables\nGroup and nest - group by the variable name\nMutate to calculate your statistics - variable ~ species\nUn-nest, filter, or select to get your desired output\n Below, I calculated p-values and R-squared values using\nstats::lm()\nstats::anova()\nbroom::tidy()\n\nNote that you could also use broom::glance() to get R-squared\n\n\npenguins %>% \n  # tidyr functions to select all the numeric columns and \n  # create a `name` and `value` column\n  pivot_longer(cols = where(is.numeric)) %>% \n  group_by(name) %>% \n  # tidyr::nest to create a data frame where each level of the \n  # grouped variable has a single row, and all the other\n  # rows and columns are now in a single nested column, `data`\n  nest() %>% \n  # use purrr::map to create new nested columns with the objects\n  # returned from `lm`, `anova`, `broom::tidy`\n  mutate(lm_fit = map(data, \n                      ~ lm(value ~ species, data = .x)),\n         r2 = map_dbl(lm_fit, ~summary(.x)$r.squared),\n         anova = map(lm_fit, anova),\n         tidied = map(anova, tidy)) %>% \n  unnest(tidied) %>%\n  # this filter removes the rows with \"Residuals\"\n  filter(term == \"species\") %>%\n  select(-data, -lm_fit, -anova) %>% \n  knitr::kable(digits = 3)\n\n\nname\nr2\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\nbill_length_mm\n0.708\nspecies\n2\n7.194317e+03\n3597.159\n410.600\n0.00\nbill_depth_mm\n0.680\nspecies\n2\n9.039670e+02\n451.984\n359.789\n0.00\nflipper_length_mm\n0.778\nspecies\n2\n5.247328e+04\n26236.642\n594.802\n0.00\nbody_mass_g\n0.670\nspecies\n2\n1.468642e+08\n73432107.078\n343.626\n0.00\nyear\n0.003\nspecies\n2\n6.010000e-01\n0.300\n0.447\n0.64\n\nWe could also do this with group_modify in dplyr.\nFrom the documentation:\n\ngroup_map(), group_modify() and group_walk() are purrr-style functions that can be used to iterate on grouped tibbles.\n\n\n\npenguins %>% \n  pivot_longer(cols = where(is.numeric)) %>% \n  group_by(name) %>% \n  # there is a litle extra work here to return r.squared\n  # group_modify needs the returned value to be a data.frame!\n  # so you need to create one\n  group_modify( ~cbind(tibble(summary(lm(value ~ species, data = .))$r.squared,\n                              .name_repair = ~c(\"r2\")),\n                       tidy(anova(lm(value ~ species, data = .))) %>%\n                         filter(term == \"species\"))) %>%\n  knitr::kable(digits = 3)\n\n\nname\nr2\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\nbill_depth_mm\n0.680\nspecies\n2\n9.039670e+02\n451.984\n359.789\n0.00\nbill_length_mm\n0.708\nspecies\n2\n7.194317e+03\n3597.159\n410.600\n0.00\nbody_mass_g\n0.670\nspecies\n2\n1.468642e+08\n73432107.078\n343.626\n0.00\nflipper_length_mm\n0.778\nspecies\n2\n5.247328e+04\n26236.642\n594.802\n0.00\nyear\n0.003\nspecies\n2\n6.010000e-01\n0.300\n0.447\n0.64\n\nAnother example: survival models\nI often work with time to event models (survival models). You can also follow this same pattern.\nTake for example the survival::lung dataset that has some variables like age, sex, performance status (ECOG and Karnofsky), etc.\n\n\nglimpse(lung)\n\n\nRows: 228\nColumns: 10\n$ inst      <dbl> 3, 3, 3, 5, 1, 12, 7, 11, 1, 7, 6, 16, 11, 21, 12,…\n$ time      <dbl> 306, 455, 1010, 210, 883, 1022, 310, 361, 218, 166…\n$ status    <dbl> 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ age       <dbl> 74, 68, 56, 57, 60, 74, 68, 71, 53, 61, 57, 68, 68…\n$ sex       <dbl> 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1,…\n$ ph.ecog   <dbl> 1, 0, 0, 1, 0, 1, 2, 2, 1, 2, 1, 2, 1, NA, 1, 1, 1…\n$ ph.karno  <dbl> 90, 90, 90, 90, 100, 50, 70, 60, 70, 70, 80, 70, 9…\n$ pat.karno <dbl> 100, 90, 90, 60, 90, 80, 60, 80, 80, 70, 80, 70, 9…\n$ meal.cal  <dbl> 1175, 1225, NA, 1150, NA, 513, 384, 538, 825, 271,…\n$ wt.loss   <dbl> NA, 15, 15, 11, 0, 0, 10, 1, 16, 34, 27, 23, 5, 32…\n\nWe can repeat the same pattern to test cox proportional hazards models for these variables individually in univariate models.\n\n\nlung %>% \n  # tidyr to make the data long\n  pivot_longer(cols = -c(status, time)) %>% \n  group_by(name) %>% \n  # group the data\n  nest() %>% \n  # use purrr::map to create new nested columns with the objects\n  mutate(cox_fit = map(data, \n                      ~ coxph(Surv(time, status) ~ value, data = .x)),\n         tidied = map(cox_fit, tidy, conf.int = TRUE)) %>% \n  unnest(tidied) %>% \n  select(-data, -cox_fit) %>% \n  knitr::kable(digits = 3)\n\n\nname\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\ninst\nvalue\n-0.010\n0.010\n-0.942\n0.346\n-0.030\n0.010\nage\nvalue\n0.019\n0.009\n2.035\n0.042\n0.001\n0.037\nsex\nvalue\n-0.531\n0.167\n-3.176\n0.001\n-0.859\n-0.203\nph.ecog\nvalue\n0.476\n0.113\n4.198\n0.000\n0.254\n0.698\nph.karno\nvalue\n-0.016\n0.006\n-2.810\n0.005\n-0.028\n-0.005\npat.karno\nvalue\n-0.020\n0.005\n-3.631\n0.000\n-0.031\n-0.009\nmeal.cal\nvalue\n0.000\n0.000\n-0.535\n0.593\n-0.001\n0.000\nwt.loss\nvalue\n0.001\n0.006\n0.217\n0.828\n-0.011\n0.013\n\n\nYou could easily add a mutate() here to calculate p-values adjusted for multiple comparisons.\nNote: edited on 2022-04-08 to fix mistake with group_modify()\nsessionInfo\n\n\npander::pander(sessionInfo())\n\n\nR version 4.0.5 (2021-03-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, utils, datasets, methods and base\nother attached packages: ggplot2(v.3.3.5), survival(v.3.2-10), palmerpenguins(v.0.1.0), broom(v.0.7.6), purrr(v.0.3.4), dplyr(v.1.0.5) and tidyr(v.1.1.3)\nloaded via a namespace (and not attached): tidyselect(v.1.1.0), xfun(v.0.30), bslib(v.0.2.5.1), pander(v.0.6.3), splines(v.4.0.5), lattice(v.0.20-41), colorspace(v.2.0-0), vctrs(v.0.3.7), generics(v.0.1.0), htmltools(v.0.5.1.1), yaml(v.2.2.1), utf8(v.1.2.1), rlang(v.0.4.10), jquerylib(v.0.1.4), pillar(v.1.6.0), glue(v.1.4.2), withr(v.2.4.2), DBI(v.1.1.1), lifecycle(v.1.0.0), stringr(v.1.4.0), munsell(v.0.5.0), gtable(v.0.3.0), memoise(v.2.0.0), evaluate(v.0.14), labeling(v.0.4.2), knitr(v.1.37), fastmap(v.1.1.0), fansi(v.0.4.2), highr(v.0.9), Rcpp(v.1.0.7), backports(v.1.2.1), scales(v.1.1.1), cachem(v.1.0.5), jsonlite(v.1.7.2), farver(v.2.1.0), distill(v.1.3), digest(v.0.6.29), stringi(v.1.5.3), grid(v.4.0.5), cli(v.3.1.0), tools(v.4.0.5), magrittr(v.2.0.1), sass(v.0.4.0), tibble(v.3.1.0), crayon(v.1.4.1), pkgconfig(v.2.0.3), downlit(v.0.4.0), ellipsis(v.0.3.1), Matrix(v.1.3-2), assertthat(v.0.2.1), rmarkdown(v.2.11), rstudioapi(v.0.13), R6(v.2.5.0) and compiler(v.4.0.5)\n\n\n\n\n",
    "preview": "posts/2022-03-20-more-models/more-models_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-04-08T10:06:22-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-03-13-stacking-in-base-r/",
    "title": "Stacking vectors",
    "description": "Don't forget to use stack().",
    "author": [
      {
        "name": "Alice",
        "url": {}
      }
    ],
    "date": "2022-03-13",
    "categories": [],
    "contents": "\nIntro\nI have recently found a couple of great use cases for the stack() function from {utils}.\nBecause I want to remind my future self about this, I thought it would make a good short post to test this {distill} site that I just created!\nDocumentation\nFrom the stack() function documentation:\n\n“Stacking vectors concatenates multiple vectors into a single vector along with a factor indicating where each observation originated. Unstacking reverses this operation.”\n\nAn example\nSometimes, I get a bunch of vectors. Maybe I had multiple files or outputs with various items in them that correspond to different groups. Often, I need to combine these and then check how many of the items exist across multiple groups.\nFor the purpose of illustration, here I will pretend that I read into R a set of gene names as a named list.\n\n\nmy_list <- list(test1 = c(\"KRAS\",\"EGFR\",\"ERBB2\"),\n                test2 = c(\"ERBB2\",\"ERBB3\",\"SPRY2\",\"AR\"),\n                test3 = c(\"APC\",\"BRAF\"))\n\n\n\nstack() makes a nice tidy data.frame! (Note that this would also work if the input was a nested list of lists.)\n\n\nstack(my_list)\n\n\n  values   ind\n1   KRAS test1\n2   EGFR test1\n3  ERBB2 test1\n4  ERBB2 test2\n5  ERBB3 test2\n6  SPRY2 test2\n7     AR test2\n8    APC test3\n9   BRAF test3\n\nIf you table() the result from stack(), now you have a nice matrix of the values in each group.\n\n\ntable(stack(my_list))\n\n\n       ind\nvalues  test1 test2 test3\n  APC       0     0     1\n  AR        0     1     0\n  BRAF      0     0     1\n  EGFR      1     0     0\n  ERBB2     1     1     0\n  ERBB3     0     1     0\n  KRAS      1     0     0\n  SPRY2     0     1     0\n\nThe resulting object is a table. You can convert it to a data.frame.\n\n\nas.data.frame.array(table(stack(my_list)))\n\n\n      test1 test2 test3\nAPC       0     0     1\nAR        0     1     0\nBRAF      0     0     1\nEGFR      1     0     0\nERBB2     1     1     0\nERBB3     0     1     0\nKRAS      1     0     0\nSPRY2     0     1     0\n\nYou can also convert the binary matrix to logical (TRUE/FALSE).\n\n\ntable(stack(my_list)) > 0\n\n\n       ind\nvalues  test1 test2 test3\n  APC   FALSE FALSE  TRUE\n  AR    FALSE  TRUE FALSE\n  BRAF  FALSE FALSE  TRUE\n  EGFR   TRUE FALSE FALSE\n  ERBB2  TRUE  TRUE FALSE\n  ERBB3 FALSE  TRUE FALSE\n  KRAS   TRUE FALSE FALSE\n  SPRY2 FALSE  TRUE FALSE\n\nNow, imagine a case where you have the table and some values are greater than 1 (because they appeared in a list more than once). You can use a trick to convert to logical and back to numeric 0/1.\n\n\nmy_list_w_repeats <- list(\n  test1 = c(\"KRAS\",\"EGFR\",\"ERBB2\"),\n  test2 = c(\"ERBB2\",\"ERBB3\",\"SPRY2\",\"AR\"),\n  test3 = c(\"APC\",\"APC\",\"APC\",\"BRAF\")) # APC is here 3 times\n\ntable(stack(my_list_w_repeats))\n\n\n       ind\nvalues  test1 test2 test3\n  APC       0     0     3\n  AR        0     1     0\n  BRAF      0     0     1\n  EGFR      1     0     0\n  ERBB2     1     1     0\n  ERBB3     0     1     0\n  KRAS      1     0     0\n  SPRY2     0     1     0\n\n+(table(stack(my_list_w_repeats)) > 0)\n\n\n       ind\nvalues  test1 test2 test3\n  APC       0     0     1\n  AR        0     1     0\n  BRAF      0     0     1\n  EGFR      1     0     0\n  ERBB2     1     1     0\n  ERBB3     0     1     0\n  KRAS      1     0     0\n  SPRY2     0     1     0\n\nSummary\nI forget about this function every once in a while and it is really useful. I also have a gist about this.\nFor fun, here is one way to do this with {dplyr} and {tidyr}. I would like to hear about other ways because I don’t find this as intuitive.\n\n\nlibrary(dplyr, quietly = TRUE)\n\nlapply(my_list, function(x) data.frame(genes = x)) %>% \n  bind_rows(.id = \"names\")\n\n\n  names genes\n1 test1  KRAS\n2 test1  EGFR\n3 test1 ERBB2\n4 test2 ERBB2\n5 test2 ERBB3\n6 test2 SPRY2\n7 test2    AR\n8 test3   APC\n9 test3  BRAF\n\nNow to make the binary matrix.\n\n\nlapply(my_list, function(x) data.frame(genes = x)) %>% \n  bind_rows(.id = \"names\") %>%\n  count(names, genes) %>%\n  tidyr::pivot_wider(names_from = \"names\",\n                     values_from = \"n\",\n                     values_fill = 0)\n\n\n# A tibble: 8 x 4\n  genes test1 test2 test3\n  <chr> <int> <int> <int>\n1 EGFR      1     0     0\n2 ERBB2     1     1     0\n3 KRAS      1     0     0\n4 AR        0     1     0\n5 ERBB3     0     1     0\n6 SPRY2     0     1     0\n7 APC       0     0     1\n8 BRAF      0     0     1\n\nsessionInfo\n\n\nsessionInfo()\n\n\nR version 4.0.5 (2021-03-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n[1] dplyr_1.0.5\n\nloaded via a namespace (and not attached):\n [1] rstudioapi_0.13   knitr_1.37        magrittr_2.0.1   \n [4] tidyselect_1.1.0  downlit_0.4.0     R6_2.5.0         \n [7] rlang_0.4.10      fastmap_1.1.0     fansi_0.4.2      \n[10] stringr_1.4.0     tools_4.0.5       xfun_0.30        \n[13] utf8_1.2.1        cli_3.1.0         DBI_1.1.1        \n[16] jquerylib_0.1.4   htmltools_0.5.1.1 ellipsis_0.3.1   \n[19] assertthat_0.2.1  yaml_2.2.1        digest_0.6.29    \n[22] tibble_3.1.0      lifecycle_1.0.0   crayon_1.4.1     \n[25] tidyr_1.1.3       purrr_0.3.4       sass_0.4.0       \n[28] vctrs_0.3.7       distill_1.3       memoise_2.0.0    \n[31] glue_1.4.2        cachem_1.0.5      evaluate_0.14    \n[34] rmarkdown_2.11    stringi_1.5.3     compiler_4.0.5   \n[37] bslib_0.2.5.1     pillar_1.6.0      generics_0.1.0   \n[40] jsonlite_1.7.2    pkgconfig_2.0.3  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-21T12:15:07-04:00",
    "input_file": {}
  }
]
